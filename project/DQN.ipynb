{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import gym\nimport numpy as np\nimport random\n\n\nclass TictactoeEnv:\n    '''\n    Description:\n        Classical Tic-tac-toe game for two players who take turns marking the spaces in a three-by-three grid with X or O.\n        The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row is the winner.\n\n        The game is played by two players: player 'X' and player 'O'. Player 'x' moves first.\n\n        The grid is represented by a 3x3 numpy array, with value in {0, 1, -1}, with corresponding values:\n            0 - place unmarked\n            1 - place marked with X\n            -1 - place marked with O\n\n        The game environment will recieve movement from two players in turn and update the grid.\n\n    self.step:\n        recieve the movement of the player, update the grid\n\n    The action space is [0-8], representing the 9 positions on the grid.\n\n    The reward is 1 if you win the game, -1 if you lose, and 0 besides.\n    '''\n\n    def __init__(self):\n        self.grid = np.zeros((3,3))\n        self.end = False\n        self.winner = None\n        self.player2value = {'X': 1, 'O': -1}\n        self.num_step = 0\n        self.current_player = 'X' # By default, player 'X' goes first\n\n    def check_valid(self, position):\n        ''' Check whether the current action is valid or not\n        '''\n        if self.end:\n            raise ValueError('This game has ended, please reset it!')\n        if type(position) is int:\n            position = (int(position / 3), position % 3)\n        elif type(position) is not tuple:\n            position = tuple(position)\n\n        return False if self.grid[position] != 0 else True\n\n    def step(self, position, print_grid=False):\n        ''' Receive the movement from two players in turn and update the grid\n        '''\n        # check the position and value are valid or not\n        # position should be a tuple like (0, 1) or int [0-8]\n        if self.end:\n            raise ValueError('This game has ended, please reset it!')\n        if type(position) is int:\n            position = (int(position / 3), position % 3)\n        elif type(position) is not tuple:\n            position = tuple(position)\n        if self.grid[position] != 0:\n            raise ValueError('There is already a chess on position {}.'.format(position))\n\n        # place a chess on the position\n        self.grid[position] = self.player2value[self.current_player]\n        # update\n        self.num_step += 1\n        self.current_player = 'X' if self.num_step % 2 == 0 else  'O'\n        # check whether the game ends or not\n        self.checkEnd()\n\n        if print_grid:\n            self.render()\n\n        return self.grid.copy(), self.end, self.winner\n\n    def get_current_player(self):\n        return self.current_player\n\n    def checkEnd(self):\n        # check rows and cols\n        if np.any(np.sum(self.grid, axis=0) == 3) or np.any(np.sum(self.grid, axis=1) == 3):\n            self.end = True\n            self.winner = 'X'\n        elif np.any(np.sum(self.grid, axis=0) == -3) or np.any(np.sum(self.grid, axis=1) == -3):\n            self.end = True\n            self.winner = 'O'\n        # check diagnols\n        elif self.grid[[0,1,2],[0,1,2]].sum() == 3 or self.grid[[0,1,2],[2,1,0]].sum() == 3:\n            self.end = True\n            self.winner = 'X'\n        elif self.grid[[0,1,2],[0,1,2]].sum() == -3 or self.grid[[0,1,2],[2,1,0]].sum() == -3:\n            self.end = True\n            self.winner = 'O'\n        # check if all the positions are filled\n        elif (self.grid == 0).sum() == 0:\n            self.end = True\n            self.winner = None # no one wins\n        else:\n            self.end = False\n            self.winner = None\n\n    def reset(self):\n        # reset the grid\n        self.grid = np.zeros((3,3))\n        self.end = False\n        self.winner = None\n        self.num_step = 0\n        self.current_player = 'X'\n\n        return self.grid.copy(), self.end, self.winner\n\n    def observe(self):\n        return self.grid.copy(), self.end, self.winner\n\n    def reward(self, player='X'):\n        if self.end:\n            if self.winner is None:\n                return 0\n            else:\n                return 1 if player == self.winner else -1\n        else:\n            return 0\n\n    def render(self):\n        # print current grid\n        value2player = {0: '-', 1: 'X', -1: 'O'}\n        for i in range(3):\n            print('|', end='')\n            for j in range(3):\n                print(value2player[int(self.grid[i,j])], end=' ' if j<2 else '')\n            print('|')\n        print()\n\nclass OptimalPlayer:\n    '''\n    Description:\n        A class to implement an epsilon-greedy optimal player in Tic-tac-toe.\n\n    About optimial policy:\n        There exists an optimial policy for game Tic-tac-toe. A player ('X' or 'O') can win or at least draw with optimial strategy.\n        See the wikipedia page for details https://en.wikipedia.org/wiki/Tic-tac-toe\n        In short, an optimal player choose the first available move from the following list:\n            [Win, BlockWin, Fork, BlockFork, Center, Corner, Side]\n\n    Parameters:\n        epsilon: float, in [0, 1]. This is a value between 0-1 that indicates the\n            probability of making a random action instead of the optimal action\n            at any given time.\n\n    '''\n    def __init__(self, epsilon=0.2, player='X'):\n        self.epsilon = epsilon\n        self.player = player # 'x' or 'O'\n\n    def set_player(self, player = 'X', j=-1):\n        self.player = player\n        if j != -1:\n            self.player = 'X' if j % 2 == 0 else 'O'\n\n    def empty(self, grid):\n        '''return all empty positions'''\n        avail = []\n        for i in range(9):\n            pos = (int(i/3), i % 3)\n            if grid[pos] == 0:\n                avail.append(pos)\n        return avail\n\n    def center(self, grid):\n        '''\n        Pick the center if its available,\n        if it's the first step of the game, center or corner are all optimial.\n        '''\n        if np.abs(grid).sum() == 0:\n            # first step of the game\n            return [(1, 1)] + self.corner(grid)\n\n        return [(1, 1)] if grid[1, 1] == 0 else []\n\n    def corner(self, grid):\n        ''' Pick empty corners to move '''\n        corner = [(0, 0), (0, 2), (2, 0), (2, 2)]\n        cn = []\n        # First, pick opposite corner of opponent if it's available\n        for i in range(4):\n            if grid[corner[i]] == 0 and grid[corner[3 - i]] != 0:\n                cn.append(corner[i])\n        if cn != []:\n            return cn\n        else:\n            for idx in corner:\n                if grid[idx] == 0:\n                    cn.append(idx)\n            return cn\n\n    def side(self, grid):\n        ''' Pick empty sides to move'''\n        rt = []\n        for idx in [(0, 1), (1, 0), (1, 2), (2, 1)]:\n            if grid[idx] == 0:\n                rt.append(idx)\n        return rt\n\n    def win(self, grid, val=None):\n        ''' Pick all positions that player will win after taking it'''\n        if val is None:\n            val = 1 if self.player == 'X' else -1\n\n        towin = []\n        # check all positions\n        for pos in self.empty(grid):\n            grid_ = np.copy(grid)\n            grid_[pos] = val\n            if self.checkWin(grid_, val):\n                towin.append(pos)\n\n        return towin\n\n    def blockWin(self, grid):\n        ''' Find the win positions of opponent and block it'''\n        oppon_val = -1 if self.player == 'X' else 1\n        return self.win(grid, oppon_val)\n\n    def fork(self, grid, val=None):\n        ''' Find a fork opportunity that the player will have two positions to win'''\n        if val is None:\n            val = 1 if self.player == 'X' else -1\n\n        tofork = []\n        # check all positions\n        for pos in self.empty(grid):\n            grid_ = np.copy(grid)\n            grid_[pos] = val\n            if self.checkFork(grid_, val):\n                tofork.append(pos)\n\n        return tofork\n\n    def blockFork(self, grid):\n        ''' Block the opponent's fork.\n            If there is only one possible fork from opponent, block it.\n            Otherwise, player should force opponent to block win by making two in a row or column\n            Amomg all possible force win positions, choose positions in opponent's fork in prior\n        '''\n        oppon_val = -1 if self.player == 'X' else 1\n        oppon_fork = self.fork(grid, oppon_val)\n        if len(oppon_fork) <= 1:\n            return oppon_fork\n\n        # force the opponent to block win\n        force_blockwin = []\n        val = 1 if self.player == 'X' else -1\n        for pos in self.empty(grid):\n            grid_ = np.copy(grid)\n            grid_[pos] = val\n            if np.any(np.sum(grid_, axis=0) == val*2) or np.any(np.sum(grid_, axis=1) == val*2):\n                force_blockwin.append(pos)\n        force_blockwin_prior = []\n        for pos in force_blockwin:\n            if pos in oppon_fork:\n                force_blockwin_prior.append(pos)\n\n        return force_blockwin_prior if force_blockwin_prior != [] else force_blockwin\n\n    def checkWin(self, grid, val=None):\n        # check whether the player corresponding to the val will win\n        if val is None:\n            val = 1 if self.player == 'X' else -1\n        target = 3 * val\n        # check rows and cols\n        if np.any(np.sum(grid, axis=0) == target) or np.any(np.sum(grid, axis=1) == target):\n            return True\n        # check diagnols\n        elif grid[[0,1,2],[0,1,2]].sum() == target or grid[[0,1,2],[2,1,0]].sum() == target:\n            return True\n        else:\n            return False\n\n    def checkFork(self, grid, val=None):\n        # check whether the player corresponding to the val will fork\n        if val is None:\n            val = 1 if self.player == 'X' else -1\n        target = 2 * val\n        # check rows and cols\n        rows = (np.sum(grid, axis=0) == target).sum()\n        cols = (np.sum(grid, axis=1) == target).sum()\n        diags = (grid[[0,1,2],[0,1,2]].sum() == target) + (grid[[0,1,2],[2,1,0]].sum() == target)\n        if (rows + cols + diags) >= 2:\n            return True\n        else:\n            return False\n\n    def randomMove(self, grid):\n        \"\"\" Chose a random move from the available options. \"\"\"\n        avail = self.empty(grid)\n\n        return avail[random.randint(0, len(avail)-1)]\n\n    def act(self, grid, **kwargs):\n        \"\"\"\n        Goes through a hierarchy of moves, making the best move that\n        is currently available each time (with probabitity 1-self.epsilon).\n        A touple is returned that represents (row, col).\n        \"\"\"\n        # whether move in random or not\n        if random.random() < self.epsilon:\n            return self.randomMove(grid)\n\n        ### optimial policies\n\n        # Win\n        win = self.win(grid)\n        if win != []:\n            return win[random.randint(0, len(win)-1)]\n        # Block win\n        block_win = self.blockWin(grid)\n        if block_win != []:\n            return block_win[random.randint(0, len(block_win)-1)]\n        # Fork\n        fork = self.fork(grid)\n        if fork != []:\n            return fork[random.randint(0, len(fork)-1)]\n        # Block fork\n        block_fork = self.blockFork(grid)\n        if block_fork != []:\n            return block_fork[random.randint(0, len(block_fork)-1)]\n        # Center\n        center = self.center(grid)\n        if center != []:\n            return center[random.randint(0, len(center)-1)]\n        # Corner\n        corner = self.corner(grid)\n        if corner != []:\n            return corner[random.randint(0, len(corner)-1)]\n        # Side\n        side = self.side(grid)\n        if side != []:\n            return side[random.randint(0, len(side)-1)]\n\n        # random move\n        return self.randomMove(grid)\n\n\n","metadata":{"id":"wIX5fbRez9V4","execution":{"iopub.status.busy":"2022-06-05T09:09:28.591932Z","iopub.execute_input":"2022-06-05T09:09:28.592498Z","iopub.status.idle":"2022-06-05T09:09:28.665705Z","shell.execute_reply.started":"2022-06-05T09:09:28.592409Z","shell.execute_reply":"2022-06-05T09:09:28.664911Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def convert(move):\n    \"\"\"\n    Convert a move in the tuple format to the int format\n    \"\"\"\n    if type(move) != tuple:\n        return move\n    else:\n        return (move[0]*3 + move[1] % 3)","metadata":{"id":"hSuxXYCvz9W2","execution":{"iopub.status.busy":"2022-06-05T09:09:28.851839Z","iopub.execute_input":"2022-06-05T09:09:28.852573Z","iopub.status.idle":"2022-06-05T09:09:28.856726Z","shell.execute_reply.started":"2022-06-05T09:09:28.852538Z","shell.execute_reply":"2022-06-05T09:09:28.855989Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Deep QL part","metadata":{"id":"g-rQD2LYz9XE"}},{"cell_type":"code","source":"import gym\nimport math\nimport random\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple, deque\nfrom itertools import count\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as T\n\n\nenv = TictactoeEnv()\n# set up matplotlib\nis_ipython = 'inline' in matplotlib.get_backend()\nif is_ipython:\n    from IPython import display\n\nplt.ion()\n\n# if gpu is to be used\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"id":"4-3yPwhZz9XF","execution":{"iopub.status.busy":"2022-06-05T09:09:29.532886Z","iopub.execute_input":"2022-06-05T09:09:29.533334Z","iopub.status.idle":"2022-06-05T09:09:32.097925Z","shell.execute_reply.started":"2022-06-05T09:09:29.533298Z","shell.execute_reply":"2022-06-05T09:09:32.096944Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"id":"cKbbZxU27LFI","outputId":"77116638-df47-4f6c-85fc-300d2428a9a1","execution":{"iopub.status.busy":"2022-06-05T09:09:32.099793Z","iopub.execute_input":"2022-06-05T09:09:32.100310Z","iopub.status.idle":"2022-06-05T09:09:32.110545Z","shell.execute_reply.started":"2022-06-05T09:09:32.100270Z","shell.execute_reply":"2022-06-05T09:09:32.109465Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Define the replay buffer","metadata":{"id":"j2FYfCsp0uT-"}},{"cell_type":"code","source":"Transition = namedtuple('Transition',\n                        ('state', 'action', 'next_state', 'reward'))\n\n\nclass ReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.memory = deque([],maxlen=capacity)\n\n    def push(self, *args):\n        \"\"\"Save a transition\"\"\"\n        self.memory.append(Transition(*args))\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)","metadata":{"id":"g41o7GeV0Q8D","execution":{"iopub.status.busy":"2022-06-05T09:09:35.444497Z","iopub.execute_input":"2022-06-05T09:09:35.444878Z","iopub.status.idle":"2022-06-05T09:09:35.451196Z","shell.execute_reply.started":"2022-06-05T09:09:35.444846Z","shell.execute_reply":"2022-06-05T09:09:35.450268Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Define the model","metadata":{"id":"YBdNrwcxEfLr"}},{"cell_type":"code","source":"class DQN(nn.Module):\n\n    def __init__(self):\n        super(DQN, self).__init__()\n        self.flatten = nn.Flatten()\n        self.lin1 = nn.Linear(18, 128)\n        self.lin2 = nn.Linear(128, 128)\n        self.lin3 = nn.Linear(128,9)\n\n    def forward(self, x):\n        x = x.to(device)\n        x = self.flatten(x).float()\n        x = F.relu(self.lin1(x))\n        x = F.relu(self.lin2(x))\n        x = self.lin3(x)\n        return x","metadata":{"id":"liW1CV3P0tE-","execution":{"iopub.status.busy":"2022-06-05T09:09:36.686420Z","iopub.execute_input":"2022-06-05T09:09:36.687210Z","iopub.status.idle":"2022-06-05T09:09:36.695032Z","shell.execute_reply.started":"2022-06-05T09:09:36.687175Z","shell.execute_reply":"2022-06-05T09:09:36.693925Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Hyperparameters","metadata":{"id":"2cWyfCOIEhWp"}},{"cell_type":"code","source":"# BATCH_SIZE = 64\n# GAMMA = 0.99\n# EPS_START = 0.9\n# EPS_END = 0.05\n# EPS_DECAY = 200\n# TARGET_UPDATE = 500\n# eps = 0.1\n\n# n_actions = 9\n\n# policy_net = DQN().to(device)\n# target_net = DQN().to(device)\n# # target_net.load_state_dict(policy_net.state_dict())\n# # target_net.eval()\n\n# optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n# memory = ReplayMemory(10000)\n\n\n# steps_done = 0\n\n\ndef select_action(state, eps = eps, target = False):\n    # epsilon-greedy choiche of the action\n    global steps_done\n    sample = random.random()\n    steps_done += 1\n    if sample > eps:\n        if target:\n            with torch.no_grad():\n                # predict the action with the actual nework\n                return target_net(state).max(1)[1].view(1, 1)            \n        else:\n            with torch.no_grad():\n                # predict the action with the actual nework\n                return policy_net(state).max(1)[1].view(1, 1)\n    else:\n        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)","metadata":{"id":"H4LoJdH1EJY1","execution":{"iopub.status.busy":"2022-06-05T09:09:52.306847Z","iopub.execute_input":"2022-06-05T09:09:52.307338Z","iopub.status.idle":"2022-06-05T09:09:52.317516Z","shell.execute_reply.started":"2022-06-05T09:09:52.307294Z","shell.execute_reply":"2022-06-05T09:09:52.316625Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Training","metadata":{"id":"fTliFEUmGa6T"}},{"cell_type":"code","source":"def optimize_model(policy_net, memory, optimizer, transition = None):\n    if(memory is None):\n        state = transition.state\n        action = transition.action\n        next_state = transition.next_state\n        reward = transition.reward\n        state_action_value = policy_net(state)[0][action].reshape(1)\n        next_state_value = 0.0 if next_state is None else target_net(next_state)[0].max(0)[0].detach()\n        expected_state_action_value = (torch.Tensor([next_state_value]) * GAMMA) + reward\n\n        criterion = nn.SmoothL1Loss()\n        loss = criterion(state_action_value, expected_state_action_value)\n\n        # Optimize the model\n        optimizer.zero_grad()\n        loss.backward()\n        # for param in policy_net.parameters():\n        #     param.grad.data.clamp_(-1, 1)\n        optimizer.step()\n        return loss\n\n    if len(memory) < BATCH_SIZE:\n        return\n    transitions = memory.sample(BATCH_SIZE)\n    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n    # detailed explanation). This converts batch-array of Transitions\n    # to Transition of batch-arrays.\n    batch = Transition(*zip(*transitions))\n    # print(f\"batch is {type(batch)} and of size {len(batch)}\")\n    # Compute a mask of non-final states and concatenate the batch elements\n    # (a final state would've been the one after which simulation ended)\n    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n                                          batch.next_state)), device=device, dtype=torch.bool)\n    non_final_next_states = torch.cat([s for s in batch.next_state\n                                                if s is not None]).view(-1, 18)\n\n    state_batch = torch.cat(batch.state)\n    action_batch = torch.cat(batch.action)\n    reward_batch = torch.cat(batch.reward).view(-1)\n\n    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n    # columns of actions taken. These are the actions which would've been taken\n    # for each batch state according to policy_net\n    state_action_values = policy_net(state_batch).gather(1, action_batch)\n    # print(\"policy_net(state_batch) \",policy_net(state_batch).shape, \"\\n\")\n    # print(policy_net(state_batch))\n    # print(\"action_batch \",action_batch.shape, \"\\n\")\n    # print(action_batch)\n    # print(\"gather \",state_action_values.shape, \"\\n\")\n    # print(state_action_values)\n\n    # Compute V(s_{t+1}) for all next states.\n    # Expected values of actions for non_final_next_states are computed based\n    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n    # This is merged based on the mask, such that we'll have either the expected\n    # state value or 0 in case the state was final.\n    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n    # Compute the expected Q values\n    expected_state_action_values = (next_state_values * GAMMA) + reward_batch.cuda()\n    # print(next_state_values.shape, \"\\n\")\n    # print(next_state_values)\n    # print(reward_batch)\n\n    # Compute Huber loss\n    criterion = nn.SmoothL1Loss()\n    # print(state_action_values, \"aaaaaaaaaa\")\n    # print(expected_state_action_values.unsqueeze(1), \"aaaaaaaaaa\")\n    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n    # Optimize the model\n    optimizer.zero_grad()\n    loss.backward()\n    for param in policy_net.parameters():\n        param.grad.data.clamp_(-1, 1)\n    optimizer.step()\n    return loss","metadata":{"id":"TdThbwInkxJP","execution":{"iopub.status.busy":"2022-06-05T09:10:50.064684Z","iopub.execute_input":"2022-06-05T09:10:50.065529Z","iopub.status.idle":"2022-06-05T09:10:50.080334Z","shell.execute_reply.started":"2022-06-05T09:10:50.065492Z","shell.execute_reply":"2022-06-05T09:10:50.079245Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def get_state(grid, q_player):\n    ones = torch.ones(1,3,3)\n    if q_player == 0:\n        state = torch.cat((ones*(grid == 1), ones*(grid == -1)))\n        return state.unsqueeze(0)\n    else:\n        state = torch.cat((ones*(grid == -1), ones*(grid == +1)))\n        return state.unsqueeze(0)\n","metadata":{"id":"H8eP1VW9kXa3","execution":{"iopub.status.busy":"2022-06-05T09:09:55.963134Z","iopub.execute_input":"2022-06-05T09:09:55.963675Z","iopub.status.idle":"2022-06-05T09:09:55.970144Z","shell.execute_reply.started":"2022-06-05T09:09:55.963639Z","shell.execute_reply":"2022-06-05T09:09:55.969192Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"results = []\nlosses = []\nfor repetition in range(10):\n    BATCH_SIZE = 64\n    GAMMA = 0.99\n    EPS_START = 0.9\n    EPS_END = 0.05\n    EPS_DECAY = 200\n    TARGET_UPDATE = 500\n    eps = 0.1\n\n    n_actions = 9\n\n    policy_net = DQN().to(device)\n    target_net = DQN().to(device)\n    target_net.load_state_dict(policy_net.state_dict())\n    target_net.eval()\n\n    optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n    memory = ReplayMemory(10000)\n\n\n    steps_done = 0\n    num_games = 20000\n    Turns = np.array(['X','O'])\n    rewards = []\n    loss_run = []\n    for i in range(num_games):\n        # Initialize the environment and state\n        print(\"iteration\", repetition,\", game \"+str(i))\n        env.reset()\n        grid, _, __ = env.observe()\n        state = get_state(grid, 1 - i%2)\n        player_opt_1 = OptimalPlayer(epsilon=0.5, player=Turns[i%2])\n        for t in range(9):\n            if env.current_player == player_opt_1.player:\n                move = convert(player_opt_1.act(grid))\n                grid, end, winner = env.step(move, print_grid=False)\n            else:\n            # Select and perform an action\n                move = select_action(state)\n                last_move_q = move\n                try:\n                    last_state_q = get_state(grid, 1 - i%2)\n                    grid, end, winner = env.step(int(move), print_grid=False)\n                    reward = torch.Tensor([env.reward(player=Turns[1 - i%2])])\n                except ValueError:\n                    reward = torch.Tensor([-1])\n                    rewards.append(-1)\n                    memory.push(last_state_q, last_move_q, None, reward)\n                    break\n\n            if not end:\n                next_state = get_state(grid, 1-i%2)\n                if env.current_player != player_opt_1.player and t>0:\n                    memory.push(last_state_q, last_move_q, next_state, reward)\n\n            else:\n                next_state = None\n                reward = env.reward(player=Turns[1 - i%2])\n                rewards.append(reward)\n                memory.push(last_state_q, last_move_q, next_state, torch.Tensor([reward]))\n                break\n\n\n\n            # print(\"pushed\", move)\n            # Move to the next state\n            state = next_state\n\n            # Perform one step of the optimization (on the policy network)\n            loss = optimize_model(policy_net, memory, optimizer)\n            loss_run.append(loss)\n\n        # Update the target network, copying all weights and biases in DQN\n        if i % 500 == 0:\n            target_net.load_state_dict(policy_net.state_dict())\n\n    results.append(rewards)\n    losses.append(loss_run)","metadata":{"id":"yNWsKptGhTL3","outputId":"76e24873-6637-4d45-cce4-eaa2f65b8693","execution":{"iopub.status.busy":"2022-06-05T08:24:59.961237Z","iopub.execute_input":"2022-06-05T08:24:59.962324Z","iopub.status.idle":"2022-06-05T08:25:06.903270Z","shell.execute_reply.started":"2022-06-05T08:24:59.962275Z","shell.execute_reply":"2022-06-05T08:25:06.901984Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"loss11 = np.array([np.array(losses[i]) for i in range(10)])","metadata":{"execution":{"iopub.status.busy":"2022-06-05T07:44:58.823326Z","iopub.execute_input":"2022-06-05T07:44:58.823711Z","iopub.status.idle":"2022-06-05T07:44:58.943932Z","shell.execute_reply.started":"2022-06-05T07:44:58.823681Z","shell.execute_reply":"2022-06-05T07:44:58.943020Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"np.save(\"Q11_res2\", results)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T07:38:11.942100Z","iopub.execute_input":"2022-06-05T07:38:11.942806Z","iopub.status.idle":"2022-06-05T07:38:11.983717Z","shell.execute_reply.started":"2022-06-05T07:38:11.942751Z","shell.execute_reply":"2022-06-05T07:38:11.982583Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"np.save(\"Q11_loss\", loss11)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T07:45:14.001432Z","iopub.execute_input":"2022-06-05T07:45:14.002287Z","iopub.status.idle":"2022-06-05T07:45:14.172919Z","shell.execute_reply.started":"2022-06-05T07:45:14.002234Z","shell.execute_reply":"2022-06-05T07:45:14.171904Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"while True:\n    continue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = []\nlosses = []\nfor repetition in range(10):\n    BATCH_SIZE = 1\n    GAMMA = 0.99\n    EPS_START = 0.9\n    EPS_END = 0.05\n    EPS_DECAY = 200\n    TARGET_UPDATE = 500\n    eps = 0.1\n\n    n_actions = 9\n\n    policy_net = DQN().to(device)\n    target_net = DQN().to(device)\n    target_net.load_state_dict(policy_net.state_dict())\n    target_net.eval()\n\n    optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n    memory = None\n\n\n    steps_done = 0\n    num_games = 20000\n    Turns = np.array(['X','O'])\n    rewards = []\n    loss_run = []\n    for i in range(num_games):\n        # Initialize the environment and state\n        print(\"iteration\", repetition,\", game \"+str(i))\n        env.reset()\n        grid, _, __ = env.observe()\n        state = get_state(grid, 1 - i%2)\n        player_opt_1 = OptimalPlayer(epsilon=0.5, player=Turns[i%2])\n        for t in range(9):\n            if env.current_player == player_opt_1.player:\n                move = convert(player_opt_1.act(grid))\n                grid, end, winner = env.step(move, print_grid=False)\n            else:\n            # Select and perform an action\n                move = select_action(state)\n                last_move_q = move\n                try:\n                    last_state_q = get_state(grid, 1 - i%2)\n                    grid, end, winner = env.step(int(move), print_grid=False)\n                    reward = torch.Tensor([env.reward(player=Turns[1 - i%2])])\n                except ValueError:\n                    reward = torch.Tensor([-1])\n                    rewards.append(-1)\n                    transition = Transition(last_state_q, last_move_q, None, reward)\n                    loss = optimize_model(policy_net, memory, optimizer, transition)\n                    loss_run.append(loss)\n                    break\n\n            if not end:\n                next_state = get_state(grid, 1-i%2)\n                if env.current_player != player_opt_1.player and t>0:\n                    transition = Transition(last_state_q, last_move_q, next_state, reward)\n                    loss = optimize_model(policy_net, memory, optimizer, transition)\n                    loss_run.append(loss)\n\n            else:\n                next_state = None\n                reward = env.reward(player=Turns[1 - i%2])\n                rewards.append(reward)\n                transition = Transition(last_state_q, last_move_q, next_state, torch.Tensor([reward]))\n                loss = optimize_model(policy_net, memory, optimizer, transition)\n                loss_run.append(loss)\n                break\n\n\n\n            # print(\"pushed\", move)\n            # Move to the next state\n            state = next_state\n\n        # Update the target network, copying all weights and biases in DQN\n        if i % 500 == 0:\n            target_net.load_state_dict(policy_net.state_dict())\n\n    results.append(rewards)\n    losses.append(loss_run)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:16:06.170505Z","iopub.execute_input":"2022-06-05T08:16:06.171002Z","iopub.status.idle":"2022-06-05T08:16:11.678067Z","shell.execute_reply.started":"2022-06-05T08:16:06.170963Z","shell.execute_reply":"2022-06-05T08:16:11.676473Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"np.save(\"Q12_res\", results)\nnp.sape(\"Q12_loss\", losses)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:47:45.630163Z","iopub.status.idle":"2022-06-04T21:47:45.630669Z","shell.execute_reply.started":"2022-06-04T21:47:45.630435Z","shell.execute_reply":"2022-06-04T21:47:45.630462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while True:\n    continue","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:47:45.632025Z","iopub.status.idle":"2022-06-04T21:47:45.632551Z","shell.execute_reply.started":"2022-06-04T21:47:45.632284Z","shell.execute_reply":"2022-06-04T21:47:45.632308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean([np.array(results)[i*250:(i+1)*250] for i in range(80)],axis=1)","metadata":{"id":"0mnGXfO-eEK4","execution":{"iopub.status.busy":"2022-06-04T21:47:45.634005Z","iopub.status.idle":"2022-06-04T21:47:45.634528Z","shell.execute_reply.started":"2022-06-04T21:47:45.634262Z","shell.execute_reply":"2022-06-04T21:47:45.634287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nmean = np.array([np.mean((np.sum(np.array(results), axis = 0)/10)[i*250:(i+1)*250]) for i in range(80)])\nvar = np.array([np.var((np.sum(np.array(results), axis = 0)/10)[i*250:(i+1)*250]) for i in range(80)])\nplt.plot(np.arange(80),mean)\nplt.fill_between(np.arange(80), mean-var/2, mean+var/2, alpha = 0.2)\nplt.title(\"Average reward over time with Q-Learning (0.1) versus Optimal Player(0.5)\")\nplt.xlabel(\"Batch of 250 games\")\nplt.ylabel(\"Average reward\")\n","metadata":{"id":"aelyV8RFsLNQ","execution":{"iopub.status.busy":"2022-06-05T07:36:21.277236Z","iopub.execute_input":"2022-06-05T07:36:21.277810Z","iopub.status.idle":"2022-06-05T07:36:26.219040Z","shell.execute_reply.started":"2022-06-05T07:36:21.277758Z","shell.execute_reply":"2022-06-05T07:36:26.217895Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nmean = np.array([np.mean((np.sum(np.array(losses), axis = 0)/5)[i*250:(i+1)*250]) for i in range(80)])\nvar = np.array([np.var((np.sum(np.array(losses), axis = 0)/5)[i*250:(i+1)*250]) for i in range(80)])\nplt.plot(np.arange(80),mean)\nplt.fill_between(np.arange(80), mean-var/2, mean+var/2, alpha = 0.2)\nplt.title(\"Average loss over time with Q-Learning (0.1) versus Optimal Player(0.5)\")\nplt.xlabel(\"Batch of 250 games\")\nplt.ylabel(\"Average loss\")","metadata":{"id":"I_ckdKgzqJZm","execution":{"iopub.status.busy":"2022-06-04T21:47:45.638001Z","iopub.status.idle":"2022-06-04T21:47:45.638523Z","shell.execute_reply.started":"2022-06-04T21:47:45.638258Z","shell.execute_reply":"2022-06-04T21:47:45.638282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ao = []\nfor i in range(80):\n    ao.append(np.mean(np.array(results[2])[i*250:(1+i)*250]))","metadata":{"id":"sEpv8ZuC7slK","execution":{"iopub.status.busy":"2022-06-04T21:47:45.639875Z","iopub.status.idle":"2022-06-04T21:47:45.640226Z","shell.execute_reply.started":"2022-06-04T21:47:45.640065Z","shell.execute_reply":"2022-06-04T21:47:45.640081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(np.arange(80),np.array(ao))","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:47:45.643190Z","iopub.status.idle":"2022-06-04T21:47:45.643884Z","shell.execute_reply.started":"2022-06-04T21:47:45.643647Z","shell.execute_reply":"2022-06-04T21:47:45.643670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q13","metadata":{}},{"cell_type":"code","source":"results = []\nlosses = []\nfor repetition in range(3):\n    BATCH_SIZE = 64\n    GAMMA = 0.99\n    max_eps = 0.8\n    min_eps = 0.1\n    n_star = 1\n    TARGET_UPDATE = 500\n\n    n_actions = 9\n\n    policy_net = DQN().to(device)\n    target_net = DQN().to(device)\n    target_net.load_state_dict(policy_net.state_dict())\n    target_net.eval()\n\n    optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n    memory = ReplayMemory(10000)\n\n\n    steps_done = 0\n    num_games = 20000\n    Turns = np.array(['X','O'])\n    winners = []\n#     loss_run = []\n    for i in range(1, num_games+1):\n        if i%250==0:\n            print(\"Testing...\")\n            current_testing = []\n            for w in range(500):\n                #Reset the environment\n                env.reset()\n                grid, _, __ = env.observe()\n                state = get_state(grid, 1 - w%2)\n                player_opt_1 = OptimalPlayer(epsilon=1., player=Turns[w%2]) #1 if rand 0 if opt\n                for t in range(9):\n                    if env.current_player == player_opt_1.player:\n                        move = convert(player_opt_1.act(grid))\n                        grid, end, winner = env.step(move, print_grid=False)\n                    else:\n                    # Select and perform an action\n                        move = select_action(state, eps=0, target = True)\n                        try:\n                            grid, end, winner = env.step(int(move), print_grid=False)\n                        except ValueError:\n                            current_testing.append(Turns[w%2])\n                            break\n\n                    if end:\n                        current_testing.append(winner)\n                        break\n            winners.append(current_testing)\n        # Initialize the environment and state\n        print(\"iteration\", repetition,\", game \"+str(i))\n        eps = max(min_eps, max_eps*(1-((i)/n_star)))\n        env.reset()\n        grid, _, __ = env.observe()\n        state = get_state(grid, 1 - i%2)\n        player_opt_1 = OptimalPlayer(epsilon=0.5, player=Turns[i%2])\n        for t in range(9):\n            if env.current_player == player_opt_1.player:\n                move = convert(player_opt_1.act(grid))\n                grid, end, winner = env.step(move, print_grid=False)\n            else:\n            # Select and perform an action\n                move = select_action(state, eps)\n                last_move_q = move\n                try:\n                    last_state_q = get_state(grid, 1 - i%2)\n                    grid, end, winner = env.step(int(move), print_grid=False)\n                    reward = torch.Tensor([env.reward(player=Turns[1 - i%2])])\n                except ValueError:\n                    reward = torch.Tensor([-1])\n                    memory.push(last_state_q, last_move_q, None, reward)\n                    break\n\n            if not end:\n                next_state = get_state(grid, 1-i%2)\n                if env.current_player != player_opt_1.player and t>0:\n                    memory.push(last_state_q, last_move_q, next_state, reward)\n\n            else:\n                next_state = None\n                reward = env.reward(player=Turns[1 - i%2])\n                memory.push(last_state_q, last_move_q, next_state, torch.Tensor([reward]))\n                break\n\n\n\n            # print(\"pushed\", move)\n            # Move to the next state\n            state = next_state\n\n            # Perform one step of the optimization (on the policy network)\n            loss = optimize_model(policy_net, memory, optimizer)\n#             loss_run.append(loss)\n\n        # Update the target network, copying all weights and biases in DQN\n        if i % 500 == 0:\n            target_net.load_state_dict(policy_net.state_dict())\n\n    results.append(winners)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T09:36:57.820880Z","iopub.execute_input":"2022-06-05T09:36:57.821408Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.array(results).shape","metadata":{"execution":{"iopub.status.busy":"2022-06-05T09:31:36.723260Z","iopub.execute_input":"2022-06-05T09:31:36.723617Z","iopub.status.idle":"2022-06-05T09:31:36.771588Z","shell.execute_reply.started":"2022-06-05T09:31:36.723587Z","shell.execute_reply":"2022-06-05T09:31:36.770643Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"np.save(\"Q13-rand-prova\", results)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T09:32:36.898520Z","iopub.execute_input":"2022-06-05T09:32:36.898891Z","iopub.status.idle":"2022-06-05T09:32:36.946428Z","shell.execute_reply.started":"2022-06-05T09:32:36.898859Z","shell.execute_reply":"2022-06-05T09:32:36.945575Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"while True:\n    continue","metadata":{"execution":{"iopub.status.busy":"2022-06-05T09:30:26.760324Z","iopub.execute_input":"2022-06-05T09:30:26.760774Z","iopub.status.idle":"2022-06-05T09:30:48.127850Z","shell.execute_reply.started":"2022-06-05T09:30:26.760723Z","shell.execute_reply":"2022-06-05T09:30:48.125730Z"},"trusted":true},"execution_count":16,"outputs":[]}]}