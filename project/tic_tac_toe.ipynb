{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wIX5fbRez9V4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQUdMCL5z9WE"
   },
   "source": [
    "# Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PAKFeeLz9WI"
   },
   "source": [
    "Our 1st game is the famous Tic Toc Toe. You can read about the game and its rules here: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "\n",
    "We implemented the game as an environment in the style of games in the [Python GYM library](https://gym.openai.com/). The commented source code is available in the file \"tic_env.py\". Here, we give a brief introduction to the environment and how it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d71xI2Kfz9WL"
   },
   "source": [
    "### Initialization and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q91zeKIRz9WM"
   },
   "source": [
    "You can initialize the environment / game as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJm7g3abz9WN"
   },
   "outputs": [],
   "source": [
    "env = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkTwtKyHz9WP"
   },
   "source": [
    "Which then has the following attributes with the corresponding initial values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFKePICVz9WR",
    "outputId": "3088b7bd-cfdd-4b62-b5ec-40fe0642fc22",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62j9rXqOz9WU"
   },
   "source": [
    "The game is played by two players: player 'X' and player 'O'. The attribute 'current_player' shows whose turn it is. We assume that player 'X' always plays first.\n",
    "\n",
    "The attribute 'grid' is a 3x3 numpy array and presents the board in the real game and the state $s_t$ in the reinfocement learning language. Each elements can take a value in {0, 1, -1}:\n",
    "     0 : place unmarked\n",
    "     1 : place marked with X \n",
    "    -1 : place marked with O \n",
    "        \n",
    "The attribute 'end' shows if the game is over or not, and the attribute 'winner' shows the winner of the game: either \"X\", \"O\", or None.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "035CW1iNz9WW"
   },
   "source": [
    "You can use function 'render' to visualize the current position of the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YG11ZIFz9WX",
    "outputId": "fca330b7-6233-4d4a-b2ce-048b40443476"
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqVJXgY1z9WY"
   },
   "source": [
    "### Taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEo5O_FCz9WZ"
   },
   "source": [
    "The game environment will recieve action from two players in turn and update the grid. At each time, one player can take the action $a_t$, where $a_t$ can either be an integer between 0 to 8 or a touple, corresponding to the 9 possible.\n",
    "\n",
    "Function 'step' is used to recieve the action of the player, update the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CALwwvbFz9Wa",
    "outputId": "7f1f1bc0-d283-46a9-a230-da7b19af4549"
   },
   "outputs": [],
   "source": [
    "env.step(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_AMsthUz9Wc",
    "outputId": "eb1312ad-bce1-46e0-eec8-397bd68c2d10"
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRmSzrKTz9Wc",
    "outputId": "82e9723d-334b-4c27-ba73-cb9492c85c80",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PO81VxEOz9We",
    "outputId": "6c09406d-6a53-45e6-9ea9-d36af38855e3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHjGOPYLz9We",
    "outputId": "05b7cd16-56dd-434d-98c2-004021402b8c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buX2MeDxz9Wf",
    "outputId": "96ee995a-fc80-486d-bb83-0ccdcf9a7d3b"
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzrQI-Acz9Wg"
   },
   "source": [
    "But not all actions are available at each time: One cannot choose a place which has been taken before. There is an error if an unavailable action is taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SiVSLn5Oz9Wh",
    "outputId": "bfd1ae0f-2cb3-49e8-f380-15c1823f2354",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWr8Igsiz9Wi"
   },
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4vvt5wbz9Wi"
   },
   "source": [
    "Reward is always 0 until the end of the game. When the game is over, the reward is 1 if you win the game, -1 if you lose, and 0 besides. Function 'observe' can be used after each step to recieve the new state $s_t$, whether the game is over, and the winner, and function 'reward' to get the reward value $r_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bRRej-az9Wk",
    "outputId": "fe3db92e-2e08-4c06-e9b6-dec45ae023b0"
   },
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbD0Jphtz9Wl",
    "outputId": "cc84567a-c5f3-4e4b-ce9e-80bfc52ba510"
   },
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PnqqIEez9Wp",
    "outputId": "1ce2c425-1973-4af4-8cba-d6bc6885f513"
   },
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WB_SqyDVz9Wp"
   },
   "source": [
    "An example of finishing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30wM16j3z9Wq",
    "outputId": "65823e58-f5f6-444f-bbcf-e5b7f52e9ffa"
   },
   "outputs": [],
   "source": [
    "env.step(0)\n",
    "env.step(3)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTn-mpvmz9Wr",
    "outputId": "b497c42a-aeec-4576-9f7a-8e8f8354945c"
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7jTKSeIz9Ws",
    "outputId": "20e0235a-a229-4335-be0e-ab729dffbfe6"
   },
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpbfnlr3z9Wt",
    "outputId": "f21c25e3-891d-4de5-988d-55b9e384fdd9"
   },
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxF88eqDz9Wt",
    "outputId": "88a01813-0467-4199-8f5f-84ef757ef753"
   },
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZqYOBO4z9Wu"
   },
   "source": [
    "# Optimal policy for Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZE3_CMLz9Wv"
   },
   "source": [
    "Fortunately, we know the exact optimal policy for Tic Toc Toe. We have implemented and $\\epsilon$-greedy version of optimal polciy which you can use for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzAZFShJz9Wv"
   },
   "outputs": [],
   "source": [
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTfHTiR5z9Ww"
   },
   "outputs": [],
   "source": [
    "opt_player = OptimalPlayer(epsilon = 0., player = 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShoegIgdz9Ww",
    "outputId": "d8bf31ac-ec63-4845-a021-cad4ff1c7b38"
   },
   "outputs": [],
   "source": [
    "opt_player.act(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPLX_qsjz9Wx",
    "outputId": "1c33e700-9bc1-4b45-f74b-d916f5119f4d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_player.player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lR7pcvExz9Wx"
   },
   "source": [
    "### An example of optimal player playing against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCN9ec_dz9Wx",
    "outputId": "331e502f-5d39-4bbb-857d-9ed4beb9acd7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(grid)\n",
    "        else:\n",
    "            move = player_rnd.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player = ' +  Turns[0])\n",
    "            print('Random player = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iEedxqOz9Wy"
   },
   "source": [
    "### An example of optimal player playing against optimal player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxzpOCeDz9Wy",
    "outputId": "07fd828a-818f-415e-fd10-637f400adb15",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt_1.act(grid)\n",
    "        else:\n",
    "            move = player_opt_2.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  Turns[0])\n",
    "            print('Optimal player 2 = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPHDtcAEz9W0"
   },
   "source": [
    "### Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANQN7UsSz9W0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "env = TictactoeEnv()\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "class Q_Player:\n",
    "    '''\n",
    "    Description:\n",
    "        A class to implement a Q-Learning optimal player in Tic-tac-toe.\n",
    "\n",
    "    Parameters:\n",
    "        epsilon: float, in [0, 1]. This is a value between 0-1 that indicates the\n",
    "            probability of making a random action instead of the optimal action\n",
    "            at any given time.\n",
    "        alpha: float, in [0, 1]. This is the learning rate\n",
    "        gamma: float, in [0, 1]. This is the discount factor\n",
    "\n",
    "    '''\n",
    "    def __init__(self, epsilon=0.2, player='X'):\n",
    "    '''\n",
    "    Description:\n",
    "        Initialize the parameters\n",
    "\n",
    "    Parameters:\n",
    "        epsilon: float, in [0, 1]. This is a value between 0-1 that indicates the\n",
    "            probability of making a random action instead of the optimal action\n",
    "            at any given time.\n",
    "        player: string, \"X\" or \"O\"\n",
    "\n",
    "    '''\n",
    "        self.epsilon = epsilon\n",
    "        self.player = player\n",
    "\n",
    "    def set_player(self, player = 'X', j=-1):\n",
    "        '''Set the player'''\n",
    "        self.player = player\n",
    "        if j != -1:\n",
    "            self.player = 'X' if j % 2 == 0 else 'O'\n",
    "\n",
    "    def empty(self, grid):\n",
    "        '''return all empty positions'''\n",
    "        avail = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i/3), i % 3)\n",
    "            if grid[pos] == 0:\n",
    "                avail.append(pos)\n",
    "        return avail\n",
    "    \n",
    "    def eps_greedyMove(self, grid, assignment, qvals, val = None):\n",
    "        \"\"\" Return the move with the optimal Q-value according to a epsilon-greedy policy. \"\"\"\n",
    "        # initialize the dictionaries hat will be updated, qvals and assignment (used to assing an ID to each state)\n",
    "        self.qvals = qvals\n",
    "        self.assignment = assignment\n",
    "        grid_id = self.assignment[str(grid)]\n",
    "        positions = np.full(9, True) #mask to set invalid positions\n",
    "        if val is None:\n",
    "            val = 1 if self.player == 'X' else -1\n",
    "        for pos in self.empty(grid):\n",
    "            grid_ = np.copy(grid)\n",
    "            grid_[pos] = val            \n",
    "            action = convert(pos)\n",
    "            positions[action] = False\n",
    "        # set to NaN invalid positions\n",
    "        self.qvals[grid_id][positions] = np.nan\n",
    "        # return the move with highest Q-value\n",
    "        best_move = int(np.nanargmax(self.qvals[grid_id]))\n",
    "        if random.random() < self.epsilon:\n",
    "            move = self.randomMove(grid)\n",
    "            return move\n",
    "        else:\n",
    "            return best_move\n",
    "        \n",
    "    def return_dicts(self):\n",
    "        \"\"\"Returns the twi updated dictionaries after each player-Q's step\"\"\"\n",
    "        return self.assignment, self.qvals\n",
    "\n",
    "    def randomMove(self, grid):\n",
    "        \"\"\" Chose a random move from the available options. \"\"\"\n",
    "        avail = self.empty(grid)\n",
    "        return avail[random.randint(0, len(avail)-1)]\n",
    "\n",
    "    def act(self, grid, assignment, qvals, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns the move in a espilon-gredy manner.\n",
    "        \"\"\"\n",
    "        return self.eps_greedyMove(grid, assignment, qvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hSuxXYCvz9W2"
   },
   "outputs": [],
   "source": [
    "def convert(move):\n",
    "    \"\"\"\n",
    "    Convert a move in the tuple format to the int format\n",
    "    \"\"\"\n",
    "    if type(move) != tuple:\n",
    "        return move\n",
    "    else:\n",
    "        return (move[0]*3 + move[1] % 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slb23Pugz9W3"
   },
   "source": [
    "## Q1 & Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rDpJGlTz9W3",
    "outputId": "b9b33c15-cbf0-4b35-ee4a-90fe67bb52f7"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for k in range(10): # we perform multiple iteartions to have robust results\n",
    "    Turns = np.array(['X','O'])\n",
    "    n_games = 20000\n",
    "    winners = []\n",
    "    count = []\n",
    "    qvals = defaultdict(lambda: np.zeros(9)) #Store the Q-values\n",
    "    assignments = defaultdict(lambda: len(assignments)) #Assign a unique ID to each state\n",
    "    alpha = 0.05\n",
    "    gamma = 0.99\n",
    "    max_eps = 0.8 # min and max epsilon for dynamic espilon\n",
    "    min_eps = 0.1\n",
    "    n_star = 20000 # Variable to be changed for Q2, in [1, 1000, 5000, 10000, 20000, 40000]\n",
    "    for i in range(n_games):\n",
    "        eps = 0.1 # max(min_eps, max_eps*(1-((i+1)/n_star))) for Q2\n",
    "        print(k,i)\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        # Initialize the two players\n",
    "        player_opt_1 = OptimalPlayer(epsilon=0.5, player=Turns[i%2])\n",
    "        player_q = Q_Player(epsilon=eps, player=Turns[1 - i%2])\n",
    "        for j in range(9):\n",
    "            # If current palyer is the optimal player\n",
    "            if env.current_player == player_opt_1.player:\n",
    "                move = player_opt_1.act(grid)\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "            \n",
    "            # If current palyer is the Q-player   \n",
    "            else:\n",
    "                move = player_q.act(grid, assignments, qvals)\n",
    "                # set the updated dictionaries as current dictionaries\n",
    "                assignments, qvals = player_q.return_dicts()\n",
    "                # Save Q-player's last move (converted to int) and last grid (state)\n",
    "                last_move_q = convert(move)\n",
    "                last_grid_q = assignments[str(grid)] # Assign/retrieve the current state's ID\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "                \n",
    "            # Q learning update for Q-values\n",
    "            if (env.current_player == player_q.player and j!=0) or end:\n",
    "                qvals[last_grid_q][last_move_q] += alpha*(env.reward(player=Turns[1 - i%2]) + gamma*np.nanmax(qvals[assignments[str(grid)]]) - qvals[last_grid_q][last_move_q])\n",
    "            \n",
    "            # If the match has ended break the loop\n",
    "            if end:\n",
    "                # Append winner to winners list\n",
    "                winners.append(winner)\n",
    "                count.append(env.reward(player=Turns[1 - i%2]))\n",
    "                env.reset()\n",
    "\n",
    "                break\n",
    "    results.append(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrRc1YyQz9W5"
   },
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvVOvZBcz9W5"
   },
   "source": [
    "### Calculations of $M_{opt}$ and $M_{rand}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKFSaolMz9W6",
    "outputId": "c2248982-c518-4d77-bb17-1a7f58cf0dd8"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for k in range(10): # we perform multiple iteartions to have robust results\n",
    "    Turns = np.array(['X','O'])\n",
    "    n_games = 20000\n",
    "    winners = []\n",
    "    count = []\n",
    "    qvals = defaultdict(lambda: np.zeros(9))  #Store the Q-values\n",
    "    assignments = defaultdict(lambda: len(assignments)) #Assign a unique ID to each state\n",
    "    alpha = 0.05\n",
    "    gamma = 0.99\n",
    "    max_eps = 0.8 # min and max epsilon for dynamic espilon\n",
    "    min_eps = 0.1\n",
    "    n_star = 30000 # Variable to be changed, in [1, 1000, 5000, 10000, 20000, 40000]\n",
    "    for i in range(1, n_games+1):\n",
    "        # TESTING PHASE\n",
    "        if i%250==0:\n",
    "            print(\"Testing...\")\n",
    "            current_testing = []\n",
    "            for w in range(500):\n",
    "                #Reset the environment\n",
    "                env.reset()\n",
    "                grid, _, __ = env.observe()\n",
    "                #Initialize the two players: Q-player with eps = 0 and Opt with eps = 0/1 depending on\n",
    "                #whether the current run is for Mopt or Mrand \n",
    "                player_opt_1 = OptimalPlayer(epsilon=1., player=Turns[w%2])\n",
    "                player_q = Q_Player(epsilon=0., player=Turns[1 - w%2])\n",
    "                for j in range(9):\n",
    "                    if env.current_player == player_opt_1.player:\n",
    "                        move = player_opt_1.act(grid)\n",
    "                        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "                    else:\n",
    "                        move = player_q.act(grid, assignments, qvals)\n",
    "                        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "                    # If the match has ended break the loop\n",
    "                    if end:\n",
    "                        current_testing.append(winner)\n",
    "                        env.reset()\n",
    "                        break\n",
    "            winners.append(current_testing)\n",
    "        # END OF TESTING               \n",
    "        eps = max(min_eps, max_eps*(1-((i)/n_star))) # Dynamic epsilon update\n",
    "        print(k,i)\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        # Training part\n",
    "        player_opt_1 = OptimalPlayer(epsilon=0.5, player=Turns[i%2])\n",
    "        player_q = Q_Player(epsilon=eps, player=Turns[1 - i%2]) # Use the dynamic epsilon\n",
    "        for j in range(9):\n",
    "            # If current player is Opt\n",
    "            if env.current_player == player_opt_1.player:\n",
    "                move = player_opt_1.act(grid)\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "               \n",
    "            # If current player is Q-player\n",
    "            else:\n",
    "                move = player_q.act(grid, assignments, qvals)\n",
    "                # set the updated dictionaries as current dictionaries\n",
    "                assignments, qvals = player_q.return_dicts()\n",
    "                # Save Q-player's last move (converted to int) and last grid (state)\n",
    "                last_move_q = convert(move)\n",
    "                last_grid_q = assignments[str(grid)]\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "            # Q-values update   \n",
    "            if (env.current_player == player_q.player and j!=0) or end:\n",
    "                qvals[last_grid_q][last_move_q] += alpha*(env.reward(player=Turns[1 - i%2]) + gamma*np.nanmax(qvals[assignments[str(grid)]]) - qvals[last_grid_q][last_move_q])\n",
    "            \n",
    "            # If the match has ended break the loop\n",
    "            if end:\n",
    "                env.reset()\n",
    "                break\n",
    "    results.append(winners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DI5ulWJUz9W8"
   },
   "source": [
    "### Calculations of $M_{rand}$ and $M_{opt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-6MVrlRz9W8",
    "outputId": "63174c50-fd18-489c-df71-d556b19004d8"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for k in range(10):\n",
    "    # Usual values initialization\n",
    "    Turns = np.array(['X','O'])\n",
    "    n_games = 20000\n",
    "    winners = []\n",
    "    count = []\n",
    "    qvals = defaultdict(lambda: np.zeros(9))  \n",
    "    assignments = defaultdict(lambda: len(assignments))\n",
    "    alpha = 0.05\n",
    "    gamma = 0.99\n",
    "    max_eps = 0.8\n",
    "    min_eps = 0.1\n",
    "    n_star = 1 # Optimal n* found in the previous question\n",
    "    eps_opt = 0 # value to be changed for the optimal player, in [0, 0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "    for i in range(1, n_games+1):\n",
    "        # TESTING PHASE\n",
    "        if i%250==0:\n",
    "            print(\"Testing...\")\n",
    "            current_testing = []\n",
    "            for w in range(500):\n",
    "                #Reset the environment\n",
    "                env.reset()\n",
    "                grid, _, __ = env.observe()\n",
    "                #Initialize the two players with epsilon 0 for Q-player and 0/1 for Optimal player\n",
    "                player_opt_1 = OptimalPlayer(epsilon=1, player=Turns[w%2])\n",
    "                player_q = Q_Player(epsilon=0., player=Turns[1 - w%2])\n",
    "                for j in range(9):\n",
    "                    # If optimal player is current player\n",
    "                    if env.current_player == player_opt_1.player:\n",
    "                        move = player_opt_1.act(grid)\n",
    "                        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "                    # If Q-player is current player    \n",
    "                    else:\n",
    "                        move = player_q.act(grid, assignments, qvals)\n",
    "                        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "                    # If the match has ended break the loop\n",
    "                    if end:\n",
    "                        # Append winner to winners list if the game is ended\n",
    "                        current_testing.append(winner)\n",
    "                        env.reset()\n",
    "                        break\n",
    "            winners.append(current_testing)\n",
    "        # END OF TESTING               \n",
    "        eps = max(min_eps, max_eps*(1-((i)/n_star))) # eps for Q-player\n",
    "        print(k,i)\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        # Initialize the two players with the correct epsilon values\n",
    "        player_opt_1 = OptimalPlayer(epsilon=eps_opt, player=Turns[i%2])\n",
    "        player_q = Q_Player(epsilon=eps, player=Turns[1 - i%2])\n",
    "        for j in range(9):\n",
    "            # If optimal player is current player\n",
    "            if env.current_player == player_opt_1.player:\n",
    "                move = player_opt_1.act(grid)\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "                \n",
    "            # If player-Q is current player   \n",
    "            else:\n",
    "                move = player_q.act(grid, assignments, qvals)\n",
    "                # set the updated dictionaries as current dictionaries\n",
    "                assignments, qvals = player_q.return_dicts()\n",
    "                # Save Q-player's last move (converted to int) and last grid (state)\n",
    "                last_move_q = convert(move)\n",
    "                last_grid_q = assignments[str(grid)]\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "            # Q-values update    \n",
    "            if (env.current_player == player_q.player and j!=0) or end:\n",
    "                qvals[last_grid_q][last_move_q] += alpha*(env.reward(player=Turns[1 - i%2]) + gamma*np.nanmax(qvals[assignments[str(grid)]]) - qvals[last_grid_q][last_move_q])\n",
    "            # If the match has ended break the loop\n",
    "            if end:\n",
    "                env.reset()\n",
    "                break\n",
    "    results.append(winners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIc-OrNsz9W_"
   },
   "source": [
    "## Q7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations of $M_{rand}$ and $M_{opt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fff2MgaTz9W_",
    "outputId": "bb36b398-f884-4d55-a6cd-4fa97c369c8b"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for k in range(10):\n",
    "    # Usual values initialization\n",
    "    Turns = np.array(['X','O'])\n",
    "    n_games = 20000\n",
    "    winners = []\n",
    "    count = []\n",
    "    qvals = defaultdict(lambda: np.zeros(9))  \n",
    "    assignments = defaultdict(lambda: len(assignments))\n",
    "    alpha = 0.05\n",
    "    gamma = 0.99\n",
    "    eps = 0.05 # Value to be changed, in [0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "    for i in range(1, n_games+1):\n",
    "        # TESTING PHASE\n",
    "        if i%250==0:\n",
    "            print(\"Testing...\")\n",
    "            current_testing = []\n",
    "            for w in range(500):\n",
    "                #Reset the environment\n",
    "                env.reset()\n",
    "                grid, _, __ = env.observe()\n",
    "                #Initialize the two players with epsilon 0 for Q-player and 0/1 for Optimal player \n",
    "                player_opt_1 = OptimalPlayer(epsilon=1, player=Turns[w%2])\n",
    "                player_q = Q_Player(epsilon=0., player=Turns[1 - w%2])\n",
    "                for j in range(9):\n",
    "                    # If optimal player is current player\n",
    "                    if env.current_player == player_opt_1.player:\n",
    "                        move = player_opt_1.act(grid)\n",
    "                        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "                    # If Q-player is current player    \n",
    "                    else:\n",
    "                        move = player_q.act(grid, assignments, qvals)\n",
    "                        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "                    if end:\n",
    "                        current_testing.append(winner)\n",
    "                        env.reset()\n",
    "                        break\n",
    "            winners.append(current_testing)\n",
    "        # END OF TESTING               \n",
    "        print(k,i)\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        # Initialize two Q-players\n",
    "        player_q_1 = Q_Player(epsilon=eps, player=Turns[i%2])\n",
    "        player_q_2 = Q_Player(epsilon=eps, player=Turns[1 - i%2])\n",
    "        for j in range(9):\n",
    "            # If current player is Q-player 1\n",
    "            if env.current_player == player_q_1.player:\n",
    "                move = player_q_1.act(grid, assignments, qvals)\n",
    "                # set the updated dictionaries as current dictionaries\n",
    "                assignments, qvals = player_q_1.return_dicts()\n",
    "                # Save Q-player 1's last move (converted to int) and last grid (state)\n",
    "                last_move_q_1 = convert(move)\n",
    "                last_grid_q_1 = assignments[str(grid)]\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "                \n",
    "            # If current player is Q-player 2\n",
    "            else:\n",
    "                move = player_q_2.act(grid, assignments, qvals)\n",
    "                # set the updated dictionaries as current dictionaries\n",
    "                assignments, qvals = player_q_2.return_dicts()\n",
    "                # Save Q-player 2's last move (converted to int) and last grid (state)\n",
    "                last_move_q_2 = convert(move)\n",
    "                last_grid_q_2 = assignments[str(grid)]\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "            # Q-values update for both players's states and actions\n",
    "            if (env.current_player == player_q_1.player and j!=0) or end:\n",
    "                qvals[last_grid_q_1][last_move_q_1] += alpha*(env.reward(player=Turns[i%2]) + gamma*np.nanmax(qvals[assignments[str(grid)]]) - qvals[last_grid_q_1][last_move_q_1])\n",
    "            elif (env.current_player == player_q_2.player and j!=0) or end:\n",
    "                qvals[last_grid_q_2][last_move_q_2] += alpha*(env.reward(player=Turns[1 - i%2]) + gamma*np.nanmax(qvals[assignments[str(grid)]]) - qvals[last_grid_q_2][last_move_q_2])\n",
    "            \n",
    "            # If the match has ended break the loop\n",
    "            if end:\n",
    "                env.reset()\n",
    "                break\n",
    "    results.append(winners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBuJbUB7z9XC"
   },
   "source": [
    "## Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations of $M_{rand}$ and $M_{opt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDS6bT07z9XC",
    "outputId": "1385b170-8c82-4416-fa87-8313c6233e76"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for k in range(10):\n",
    "    # Usual values initialization\n",
    "    Turns = np.array(['X','O'])\n",
    "    n_games = 10000\n",
    "    winners = []\n",
    "    count = []\n",
    "    qvals = defaultdict(lambda: np.zeros(9))  \n",
    "    assignments = defaultdict(lambda: len(assignments))\n",
    "    alpha = 0.05\n",
    "    gamma = 0.99\n",
    "    max_eps = 0.8\n",
    "    min_eps = 0.1\n",
    "    n_star = 1 #Value to be changed, in [1 1000 5000 10000 20000 40000]\n",
    "    for i in range(1, n_games+1):\n",
    "        # TESTING PHASE\n",
    "        if i%250==0:\n",
    "            print(\"Testing...\")\n",
    "            current_testing = []\n",
    "            for w in range(500):\n",
    "                #Reset the environment\n",
    "                env.reset()\n",
    "                grid, _, __ = env.observe()\n",
    "                #Initialize the two players with epsilon 0 for Q-player and 0/1 for Optimal player \n",
    "                player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[w%2])\n",
    "                player_q = Q_Player(epsilon=0., player=Turns[1 - w%2])\n",
    "                for j in range(9):\n",
    "                    if env.current_player == player_opt_1.player:\n",
    "                        move = player_opt_1.act(grid)\n",
    "                        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "                    else:\n",
    "                        move = player_q.act(grid, assignments, qvals)\n",
    "                        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "                    if end:\n",
    "                        current_testing.append(winner)\n",
    "                        env.reset()\n",
    "                        break\n",
    "            winners.append(current_testing)\n",
    "        # END OF TESTING               \n",
    "        eps = max(min_eps, max_eps*(1-((i)/n_star))) #dynamic epsilon update\n",
    "        print(k,i)\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        # Initialize two Q-players\n",
    "        player_q_1 = Q_Player(epsilon=eps, player=Turns[i%2])\n",
    "        player_q_2 = Q_Player(epsilon=eps, player=Turns[1 - i%2])\n",
    "        for j in range(9):\n",
    "            # If current player is Q-player 1\n",
    "            if env.current_player == player_q_1.player:\n",
    "                move = player_q_1.act(grid, assignments, qvals)\n",
    "                # set the updated dictionaries as current dictionaries\n",
    "                assignments, qvals = player_q_1.return_dicts()\n",
    "                # Save Q-player 1's last move (converted to int) and last grid (state)\n",
    "                last_move_q_1 = convert(move)\n",
    "                last_grid_q_1 = assignments[str(grid)]\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "             \n",
    "            # If current player is Q-player 2\n",
    "            else:\n",
    "                move = player_q_2.act(grid, assignments, qvals)\n",
    "                # set the updated dictionaries as current dictionaries\n",
    "                assignments, qvals = player_q_2.return_dicts()\n",
    "                # Save Q-player 2's last move (converted to int) and last grid (state)\n",
    "                last_move_q_2 = convert(move)\n",
    "                last_grid_q_2 = assignments[str(grid)]\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "            # Q-values update for both players's states and actions    \n",
    "            if (env.current_player == player_q_1.player and j!=0) or end:\n",
    "                qvals[last_grid_q_1][last_move_q_1] += alpha*(env.reward(player=Turns[i%2]) + gamma*np.nanmax(qvals[assignments[str(grid)]]) - qvals[last_grid_q_1][last_move_q_1])\n",
    "            elif (env.current_player == player_q_2.player and j!=0) or end:\n",
    "                qvals[last_grid_q_2][last_move_q_2] += alpha*(env.reward(player=Turns[1 - i%2]) + gamma*np.nanmax(qvals[assignments[str(grid)]]) - qvals[last_grid_q_2][last_move_q_2])\n",
    "            \n",
    "            # If the match has ended break the loop\n",
    "            if end:\n",
    "                env.reset()\n",
    "                break\n",
    "    results.append(winners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-rQD2LYz9XE"
   },
   "source": [
    "## Deep QL part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section has been written followin Pytorch's tutorial on reinforcement learning (DQN).\n",
    "\n",
    "Source: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4-3yPwhZz9XF"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from tic_env import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = TictactoeEnv()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2FYfCsp0uT-"
   },
   "source": [
    "Define the replay buffer that will be used for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "g41o7GeV0Q8D"
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a random sample from the buffer, of size batch size\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBdNrwcxEfLr"
   },
   "source": [
    "Define the model, following the instructions given in the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "liW1CV3P0tE-"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.flatten = nn.Flatten() # First flatten the input from (batch_size,2,3,3) to (batch_size,18)\n",
    "        self.lin1 = nn.Linear(18, 128) # First hidden linear layer\n",
    "        self.lin2 = nn.Linear(128, 128) # Second hidden linear layer\n",
    "        self.lin3 = nn.Linear(128,9) # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.flatten(x).float()\n",
    "        x = F.relu(self.lin1(x)) # Apply RElU activation function\n",
    "        x = F.relu(self.lin2(x)) # Apply RElU activation function\n",
    "        x = self.lin3(x) # No activation funciton for output layer (i. e. linear activation funciton)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "H4LoJdH1EJY1"
   },
   "outputs": [],
   "source": [
    "def select_action(state, eps = 0.1, target = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns a move chosen according to a epsilon-greedy policy\n",
    "    \n",
    "    Parameters:\n",
    "        state: Tensor representing the current grid\n",
    "        eps: float\n",
    "        target: boolean, set to False during train and to True during test\n",
    "    \"\"\"\n",
    "    # epsilon-greedy choiche of the action\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    if sample > eps:\n",
    "        # Return a move according to NN prediction\n",
    "        if target:\n",
    "            with torch.no_grad():\n",
    "                # predict the action with the target nework (for testing phase)\n",
    "                return target_net(state).max(1)[1].view(1, 1)            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # predict the action with the actual nework\n",
    "                return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        # Return random move\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTliFEUmGa6T"
   },
   "source": [
    "Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TdThbwInkxJP"
   },
   "outputs": [],
   "source": [
    "def optimize_model(policy_net, memory, optimizer, transition = None):\n",
    "    \"\"\"Optimizer for training part\"\"\"\n",
    "    # case in which the memory is not used (i.e. only the last transition is considered)\n",
    "    if memory is None :\n",
    "        state = transition.state\n",
    "        action = transition.action\n",
    "        next_state = transition.next_state\n",
    "        reward = transition.reward\n",
    "        state_action_value = policy_net(state)[0][action].reshape(1)\n",
    "        # filter out None next states, i.e. states after the end of a match\n",
    "        if next_state is None:\n",
    "            next_state_value = 0.0\n",
    "        else:\n",
    "            next_state_value = target_net(next_state)[0].max(0)[0].detach()\n",
    "        # compute expected value\n",
    "        expected_state_action_value = (torch.Tensor([next_state_value]) * GAMMA) + reward\n",
    "\n",
    "        # Huber loss to be optimized\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_value, expected_state_action_value)\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    # Go from batch-array of Transitions to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    # Compute a mask of non-none (final) states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]).view(-1, 18)\n",
    "\n",
    "    # Aggregate states, actions and reward from the same batch\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward).view(-1) #reshape the reward to be coherent with other values\n",
    "\n",
    "    # Compute the predicted Q-values\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Predict with the target net the non-final states\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch#.cuda()\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define what is a state in the current setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "H8eP1VW9kXa3"
   },
   "outputs": [],
   "source": [
    "def get_state(grid, q_player):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Function that returns the correct form of state\n",
    "    \n",
    "    Parameters:\n",
    "        grid: numpy array, represent the current grid of the game\n",
    "        q_player: int, 0 or 1 indicating from which player's point of view to return the state\n",
    "        \n",
    "    Output:\n",
    "        Tensor of size (1,2,3,3)\n",
    "    \"\"\"\n",
    "    ones = torch.ones(1,3,3)\n",
    "    if q_player == 0:\n",
    "        state = torch.cat((ones*(grid == 1), ones*(grid == -1)))\n",
    "        return state.unsqueeze(0)\n",
    "    else:\n",
    "        state = torch.cat((ones*(grid == -1), ones*(grid == +1)))\n",
    "        return state.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yNWsKptGhTL3",
    "outputId": "76e24873-6637-4d45-cce4-eaa2f65b8693"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "losses = []\n",
    "for repetition in range(1):\n",
    "    BATCH_SIZE = 64 #batch size for the memory buffer\n",
    "    GAMMA = 0.99\n",
    "    eps = 0.1\n",
    "    n_actions = 9\n",
    "\n",
    "    #Initialize the policy and target networks\n",
    "    policy_net = DQN().to(device)\n",
    "    target_net = DQN().to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    \n",
    "    #Initialize the optimizer and the memory buffer\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n",
    "    memory = ReplayMemory(10000)\n",
    "\n",
    "    num_games = 20000\n",
    "    Turns = np.array(['X','O'])\n",
    "    rewards = []\n",
    "    loss_run = [] # save the losses of each run of 20000 games\n",
    "    for i in range(num_games):\n",
    "        game_loss = [] # save the losses of the current game\n",
    "        # Initialize the environment and state\n",
    "        print(\"iteration\", repetition,\", game \"+str(i))\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        # Set the initial state from Q-player's point of view\n",
    "        state = get_state(grid, 1 - i%2)\n",
    "        # Initialize the optimal player\n",
    "        player_opt_1 = OptimalPlayer(epsilon=0.5, player=Turns[i%2])\n",
    "        for t in range(9):\n",
    "            # If optimal player is current player\n",
    "            if env.current_player == player_opt_1.player:\n",
    "                move = convert(player_opt_1.act(grid))\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "            else:\n",
    "                # Select an action for Q-player\n",
    "                move = select_action(state)\n",
    "                # save Q-player's last move\n",
    "                last_move_q = move\n",
    "                # Try to do the chosen move\n",
    "                try:\n",
    "                    # If valid, do the move and save the state and the reward\n",
    "                    last_state_q = get_state(grid, 1 - i%2)\n",
    "                    grid, end, winner = env.step(int(move), print_grid=False)\n",
    "                    reward = torch.Tensor([env.reward(player=Turns[1 - i%2])])\n",
    "                except ValueError:\n",
    "                    # If invalid move, set reward to -1, end the game and push to memory\n",
    "                    reward = torch.Tensor([-1])\n",
    "                    rewards.append(-1)\n",
    "                    memory.push(last_state_q, last_move_q, None, reward)\n",
    "                    break\n",
    "\n",
    "            # When it's Q-player's turn and it's not the first move neither the end, push to memory buffer\n",
    "            if not end:\n",
    "                next_state = get_state(grid, 1-i%2)\n",
    "                if env.current_player != player_opt_1.player and t>0:\n",
    "                    memory.push(last_state_q, last_move_q, next_state, reward)\n",
    "\n",
    "            # If game has ended, push to memory, set to None next state\n",
    "            else:\n",
    "                next_state = None\n",
    "                reward = env.reward(player=Turns[1 - i%2])\n",
    "                rewards.append(reward)\n",
    "                memory.push(last_state_q, last_move_q, next_state, torch.Tensor([reward]))\n",
    "                break\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss = optimize_model(policy_net, memory, optimizer)\n",
    "            if loss == None:\n",
    "                game_loss.append(None)\n",
    "            else:\n",
    "                game_loss.append(loss.item())\n",
    "                \n",
    "        loss_run.append(game_loss)\n",
    "        # Update the target network, copying all weights and biases of policy_net\n",
    "        if i % 500 == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    results.append(rewards)\n",
    "    losses.append(loss_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEpv8ZuC7slK"
   },
   "source": [
    "## Q12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DQN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-76c50aa4c846>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mn_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mpolicy_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mtarget_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtarget_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DQN' is not defined"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "losses = []\n",
    "for repetition in range(1):\n",
    "    # Initialize the parameters and the nural networks\n",
    "    BATCH_SIZE = 1 # In this case batch_size = 1\n",
    "    GAMMA = 0.99\n",
    "    eps = 0.1\n",
    "\n",
    "    n_actions = 9\n",
    "\n",
    "    policy_net = DQN().to(device)\n",
    "    target_net = DQN().to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n",
    "    memory = None # Set memory to None (i. e. use only the last transition)\n",
    "\n",
    "    num_games = 20000\n",
    "    Turns = np.array(['X','O'])\n",
    "    rewards = []\n",
    "    loss_run = []\n",
    "    for i in range(num_games):\n",
    "        loss_game = []\n",
    "        # Initialize the environment and state\n",
    "        print(\"iteration\", repetition,\", game \"+str(i))\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        # Set the initial state from Q-player's point of view\n",
    "        state = get_state(grid, 1 - i%2)\n",
    "        # Initialize the optimal player\n",
    "        player_opt_1 = OptimalPlayer(epsilon=0.5, player=Turns[i%2])\n",
    "        for t in range(9):\n",
    "            # If optimal player is current player\n",
    "            if env.current_player == player_opt_1.player:\n",
    "                move = convert(player_opt_1.act(grid))\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "            \n",
    "            else:\n",
    "                # Select an action for Q-player\n",
    "                move = select_action(state)\n",
    "                # Save Q-player's last move\n",
    "                last_move_q = move\n",
    "                try:\n",
    "                    # If valid, do the move and save the state and the reward\n",
    "                    last_state_q = get_state(grid, 1 - i%2)\n",
    "                    grid, end, winner = env.step(int(move), print_grid=False)\n",
    "                    reward = torch.Tensor([env.reward(player=Turns[1 - i%2])])\n",
    "                except ValueError:\n",
    "                    # If invalid move, set reward to -1, end the game and optimize\n",
    "                    reward = torch.Tensor([-1])\n",
    "                    rewards.append(-1)\n",
    "                    # Specify the transition that will be used in the optimization part, as now we use only the last one\n",
    "                    transition = Transition(last_state_q, last_move_q, None, reward)\n",
    "                    loss = optimize_model(policy_net, memory, optimizer, transition)\n",
    "                    if loss == None:\n",
    "                        loss_game.append(loss)\n",
    "                    else:\n",
    "                        loss_game.append(loss.item())\n",
    "                    break\n",
    "\n",
    "            # When it's Q-player's turn and it's not the first move neither the end, optimize using last transition\n",
    "            if not end:\n",
    "                next_state = get_state(grid, 1-i%2)\n",
    "                if env.current_player != player_opt_1.player and t>0:\n",
    "                    transition = Transition(last_state_q, last_move_q, next_state, reward)\n",
    "                    loss = optimize_model(policy_net, memory, optimizer, transition)\n",
    "                    if loss == None:\n",
    "                        loss_game.append(loss)\n",
    "                    else:\n",
    "                        loss_game.append(loss.item())\n",
    "            \n",
    "            # If game has ended, optimize, set to None next state\n",
    "            else:\n",
    "                next_state = None\n",
    "                reward = env.reward(player=Turns[1 - i%2])\n",
    "                rewards.append(reward)\n",
    "                transition = Transition(last_state_q, last_move_q, next_state, torch.Tensor([reward]))\n",
    "                loss = optimize_model(policy_net, memory, optimizer, transition)\n",
    "                if loss == None:\n",
    "                    loss_game.append(loss)\n",
    "                else:\n",
    "                    loss_game.append(loss.item())\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        loss_run.append(loss_game)\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if i % 500 == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    results.append(rewards)\n",
    "    losses.append(loss_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations of $M_{rand}$ and $M_{opt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [1, 1000, 5000, 10000, 15000, 20000, 40000]:  # possible values of n* to try  \n",
    "    results = []\n",
    "    for repetition in range(1):\n",
    "        #set the required parameters and the neural netwokrs\n",
    "        BATCH_SIZE = 64\n",
    "        GAMMA = 0.99\n",
    "        max_eps = 0.8\n",
    "        min_eps = 0.1\n",
    "        n_star = n\n",
    "        TARGET_UPDATE = 500\n",
    "\n",
    "        n_actions = 9\n",
    "\n",
    "        policy_net = DQN().to(device)\n",
    "        target_net = DQN().to(device)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "\n",
    "        # Initialize the optimizer and the memory buffer\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n",
    "        memory = ReplayMemory(10000)\n",
    "\n",
    "        num_games = 20000\n",
    "        Turns = np.array(['X','O'])\n",
    "        winners = []\n",
    "        for i in range(1, num_games+1):\n",
    "            # Testing phase\n",
    "            if i%250==0:\n",
    "                print(\"Testing...\")\n",
    "                current_testing = []\n",
    "                for w in range(500):\n",
    "                    #Reset the environment\n",
    "                    env.reset()\n",
    "                    grid, _, __ = env.observe()\n",
    "                    #Initialize the Q-player and the optimal player\n",
    "                    state = get_state(grid, 1 - w%2)\n",
    "                    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[w%2]) #1 if rand 0 if opt\n",
    "                    for t in range(9):\n",
    "                        if env.current_player == player_opt_1.player:\n",
    "                            move = convert(player_opt_1.act(grid))\n",
    "                            grid, end, winner = env.step(move, print_grid=False)\n",
    "                        else:\n",
    "                            # Select an action according to policy_net, setting eps = 0 and target = True\n",
    "                            move = select_action(get_state(grid, 1-w%2), eps=0, target = True)\n",
    "                            # Try the chosen move\n",
    "                            try:\n",
    "                                grid, end, winner = env.step(int(move), print_grid=False)\n",
    "                            except ValueError:\n",
    "                                current_testing.append(Turns[w%2])\n",
    "                                break\n",
    "\n",
    "                        if end:\n",
    "                            current_testing.append(winner)\n",
    "                            break\n",
    "                winners.append(current_testing)\n",
    "            # Testing ended\n",
    "            # Initialize the environment and state\n",
    "            print(\"iteration\", repetition,\", game \"+str(i))\n",
    "            eps = max(min_eps, max_eps*(1-((i)/n_star))) # Dynamic epsilon update\n",
    "            env.reset()\n",
    "            grid, _, __ = env.observe()\n",
    "            # Set state and optimal player\n",
    "            state = get_state(grid, 1 - i%2)\n",
    "            player_opt_1 = OptimalPlayer(epsilon=0.5, player=Turns[i%2])\n",
    "            for t in range(9):\n",
    "                if env.current_player == player_opt_1.player:\n",
    "                    move = convert(player_opt_1.act(grid))\n",
    "                    grid, end, winner = env.step(move, print_grid=False)\n",
    "                else:\n",
    "                    # Select and try an action\n",
    "                    move = select_action(state, eps)\n",
    "                    last_move_q = move\n",
    "                    try:\n",
    "                        last_state_q = get_state(grid, 1 - i%2)\n",
    "                        grid, end, winner = env.step(int(move), print_grid=False)\n",
    "                        reward = torch.Tensor([env.reward(player=Turns[1 - i%2])])\n",
    "                    except ValueError:\n",
    "                        reward = torch.Tensor([-1])\n",
    "                        memory.push(last_state_q, last_move_q, None, reward)\n",
    "                        break\n",
    "\n",
    "                # When it's Q-player's turn and it's not the first move neither the end, push to memory buffer\n",
    "                if not end:\n",
    "                    next_state = get_state(grid, 1-i%2)\n",
    "                    if env.current_player != player_opt_1.player and t>0:\n",
    "                        memory.push(last_state_q, last_move_q, next_state, reward)\n",
    "\n",
    "                # If game has ended, push to memory, set to None next state\n",
    "                else:\n",
    "                    next_state = None\n",
    "                    reward = env.reward(player=Turns[1 - i%2])\n",
    "                    memory.push(last_state_q, last_move_q, next_state, torch.Tensor([reward]))\n",
    "                    break\n",
    "\n",
    "                # Update current state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                loss = optimize_model(policy_net, memory, optimizer)\n",
    "\n",
    "            # Update the target network, copying all weights and biases from policy_net\n",
    "            if i % 500 == 0:\n",
    "                print(\"loaded\")\n",
    "                target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations of $M_{rand}$ and $M_{opt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 , game 1\n",
      "iteration 0 , game 2\n",
      "iteration 0 , game 3\n",
      "iteration 0 , game 4\n",
      "iteration 0 , game 5\n",
      "iteration 0 , game 6\n",
      "iteration 0 , game 7\n",
      "iteration 0 , game 8\n",
      "iteration 0 , game 9\n",
      "iteration 0 , game 10\n",
      "iteration 0 , game 11\n",
      "iteration 0 , game 12\n",
      "iteration 0 , game 13\n",
      "iteration 0 , game 14\n",
      "iteration 0 , game 15\n",
      "iteration 0 , game 16\n",
      "iteration 0 , game 17\n",
      "iteration 0 , game 18\n",
      "iteration 0 , game 19\n",
      "iteration 0 , game 20\n",
      "iteration 0 , game 21\n",
      "iteration 0 , game 22\n",
      "iteration 0 , game 23\n",
      "iteration 0 , game 24\n",
      "iteration 0 , game 25\n",
      "iteration 0 , game 26\n",
      "iteration 0 , game 27\n",
      "iteration 0 , game 28\n",
      "iteration 0 , game 29\n",
      "iteration 0 , game 30\n",
      "iteration 0 , game 31\n",
      "iteration 0 , game 32\n",
      "iteration 0 , game 33\n",
      "iteration 0 , game 34\n",
      "iteration 0 , game 35\n",
      "iteration 0 , game 36\n",
      "iteration 0 , game 37\n",
      "iteration 0 , game 38\n",
      "iteration 0 , game 39\n",
      "iteration 0 , game 40\n",
      "iteration 0 , game 41\n",
      "iteration 0 , game 42\n",
      "iteration 0 , game 43\n",
      "iteration 0 , game 44\n",
      "iteration 0 , game 45\n",
      "iteration 0 , game 46\n",
      "iteration 0 , game 47\n",
      "iteration 0 , game 48\n",
      "iteration 0 , game 49\n",
      "iteration 0 , game 50\n",
      "iteration 0 , game 51\n",
      "iteration 0 , game 52\n",
      "iteration 0 , game 53\n",
      "iteration 0 , game 54\n",
      "iteration 0 , game 55\n",
      "iteration 0 , game 56\n",
      "iteration 0 , game 57\n",
      "iteration 0 , game 58\n",
      "iteration 0 , game 59\n",
      "iteration 0 , game 60\n",
      "iteration 0 , game 61\n",
      "iteration 0 , game 62\n",
      "iteration 0 , game 63\n",
      "iteration 0 , game 64\n",
      "iteration 0 , game 65\n",
      "iteration 0 , game 66\n",
      "iteration 0 , game 67\n",
      "iteration 0 , game 68\n",
      "iteration 0 , game 69\n",
      "iteration 0 , game 70\n",
      "iteration 0 , game 71\n",
      "iteration 0 , game 72\n",
      "iteration 0 , game 73\n",
      "iteration 0 , game 74\n",
      "iteration 0 , game 75\n",
      "iteration 0 , game 76\n",
      "iteration 0 , game 77\n",
      "iteration 0 , game 78\n",
      "iteration 0 , game 79\n",
      "iteration 0 , game 80\n",
      "iteration 0 , game 81\n",
      "iteration 0 , game 82\n",
      "iteration 0 , game 83\n",
      "iteration 0 , game 84\n",
      "iteration 0 , game 85\n",
      "iteration 0 , game 86\n",
      "iteration 0 , game 87\n",
      "iteration 0 , game 88\n",
      "iteration 0 , game 89\n",
      "iteration 0 , game 90\n",
      "iteration 0 , game 91\n",
      "iteration 0 , game 92\n",
      "iteration 0 , game 93\n",
      "iteration 0 , game 94\n",
      "iteration 0 , game 95\n",
      "iteration 0 , game 96\n",
      "iteration 0 , game 97\n",
      "iteration 0 , game 98\n",
      "iteration 0 , game 99\n",
      "iteration 0 , game 100\n",
      "iteration 0 , game 101\n",
      "iteration 0 , game 102\n",
      "iteration 0 , game 103\n",
      "iteration 0 , game 104\n",
      "iteration 0 , game 105\n",
      "iteration 0 , game 106\n",
      "iteration 0 , game 107\n",
      "iteration 0 , game 108\n",
      "iteration 0 , game 109\n",
      "iteration 0 , game 110\n",
      "iteration 0 , game 111\n",
      "iteration 0 , game 112\n",
      "iteration 0 , game 113\n",
      "iteration 0 , game 114\n",
      "iteration 0 , game 115\n",
      "iteration 0 , game 116\n",
      "iteration 0 , game 117\n",
      "iteration 0 , game 118\n",
      "iteration 0 , game 119\n",
      "iteration 0 , game 120\n",
      "iteration 0 , game 121\n",
      "iteration 0 , game 122\n",
      "iteration 0 , game 123\n",
      "iteration 0 , game 124\n",
      "iteration 0 , game 125\n",
      "iteration 0 , game 126\n",
      "iteration 0 , game 127\n",
      "iteration 0 , game 128\n",
      "iteration 0 , game 129\n",
      "iteration 0 , game 130\n",
      "iteration 0 , game 131\n",
      "iteration 0 , game 132\n",
      "iteration 0 , game 133\n",
      "iteration 0 , game 134\n",
      "iteration 0 , game 135\n",
      "iteration 0 , game 136\n",
      "iteration 0 , game 137\n",
      "iteration 0 , game 138\n",
      "iteration 0 , game 139\n",
      "iteration 0 , game 140\n",
      "iteration 0 , game 141\n",
      "iteration 0 , game 142\n",
      "iteration 0 , game 143\n",
      "iteration 0 , game 144\n",
      "iteration 0 , game 145\n",
      "iteration 0 , game 146\n",
      "iteration 0 , game 147\n",
      "iteration 0 , game 148\n",
      "iteration 0 , game 149\n",
      "iteration 0 , game 150\n",
      "iteration 0 , game 151\n",
      "iteration 0 , game 152\n",
      "iteration 0 , game 153\n",
      "iteration 0 , game 154\n",
      "iteration 0 , game 155\n",
      "iteration 0 , game 156\n",
      "iteration 0 , game 157\n",
      "iteration 0 , game 158\n",
      "iteration 0 , game 159\n",
      "iteration 0 , game 160\n",
      "iteration 0 , game 161\n",
      "iteration 0 , game 162\n",
      "iteration 0 , game 163\n",
      "iteration 0 , game 164\n",
      "iteration 0 , game 165\n",
      "iteration 0 , game 166\n",
      "iteration 0 , game 167\n",
      "iteration 0 , game 168\n",
      "iteration 0 , game 169\n",
      "iteration 0 , game 170\n",
      "iteration 0 , game 171\n",
      "iteration 0 , game 172\n",
      "iteration 0 , game 173\n",
      "iteration 0 , game 174\n",
      "iteration 0 , game 175\n",
      "iteration 0 , game 176\n",
      "iteration 0 , game 177\n",
      "iteration 0 , game 178\n",
      "iteration 0 , game 179\n",
      "iteration 0 , game 180\n",
      "iteration 0 , game 181\n",
      "iteration 0 , game 182\n",
      "iteration 0 , game 183\n",
      "iteration 0 , game 184\n",
      "iteration 0 , game 185\n",
      "iteration 0 , game 186\n",
      "iteration 0 , game 187\n",
      "iteration 0 , game 188\n",
      "iteration 0 , game 189\n",
      "iteration 0 , game 190\n",
      "iteration 0 , game 191\n",
      "iteration 0 , game 192\n",
      "iteration 0 , game 193\n",
      "iteration 0 , game 194\n",
      "iteration 0 , game 195\n",
      "iteration 0 , game 196\n",
      "iteration 0 , game 197\n",
      "iteration 0 , game 198\n",
      "iteration 0 , game 199\n",
      "iteration 0 , game 200\n",
      "iteration 0 , game 201\n",
      "iteration 0 , game 202\n",
      "iteration 0 , game 203\n",
      "iteration 0 , game 204\n",
      "iteration 0 , game 205\n",
      "iteration 0 , game 206\n",
      "iteration 0 , game 207\n",
      "iteration 0 , game 208\n",
      "iteration 0 , game 209\n",
      "iteration 0 , game 210\n",
      "iteration 0 , game 211\n",
      "iteration 0 , game 212\n",
      "iteration 0 , game 213\n",
      "iteration 0 , game 214\n",
      "iteration 0 , game 215\n",
      "iteration 0 , game 216\n",
      "iteration 0 , game 217\n",
      "iteration 0 , game 218\n",
      "iteration 0 , game 219\n",
      "iteration 0 , game 220\n",
      "iteration 0 , game 221\n",
      "iteration 0 , game 222\n",
      "iteration 0 , game 223\n",
      "iteration 0 , game 224\n",
      "iteration 0 , game 225\n",
      "iteration 0 , game 226\n",
      "iteration 0 , game 227\n",
      "iteration 0 , game 228\n",
      "iteration 0 , game 229\n",
      "iteration 0 , game 230\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-30291b6b86ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;31m# Perform one step of the optimization (on the policy network)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;31m# Update the target network, copying all weights and biases from policy_net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-a3d55a5bd375>\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m(policy_net, memory, optimizer, transition)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m# Compute the predicted Q-values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mstate_action_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# Predict with the target net the non-final states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-d116f0d3b0b2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Apply RElU activation function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Apply RElU activation function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# No activation funciton for output layer (i. e. linear activation funciton)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epsilon in [0, 0.1, 0.25, 0.5, 0.75, 0.9]: # Different values of epsilon to try\n",
    "    results = []\n",
    "    for repetition in range(1):\n",
    "        BATCH_SIZE = 64\n",
    "        GAMMA = 0.99\n",
    "        max_eps = 0.8\n",
    "        min_eps = 0.1\n",
    "        n_star = 1 # Value found in the previous exercise\n",
    "        TARGET_UPDATE = 500\n",
    "\n",
    "        n_actions = 9\n",
    "\n",
    "        policy_net = DQN().to(device)\n",
    "        target_net = DQN().to(device)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n",
    "        memory = ReplayMemory(10000)\n",
    "\n",
    "        num_games = 20000\n",
    "        Turns = np.array(['X','O'])\n",
    "        winners = []\n",
    "        for i in range(1, num_games+1):\n",
    "            # Testing phase\n",
    "            if i%250==0:\n",
    "                print(\"Testing...\")\n",
    "                current_testing = []\n",
    "                for w in range(500):\n",
    "                    #Reset the environment\n",
    "                    env.reset()\n",
    "                    grid, _, __ = env.observe()\n",
    "                    # Set current state and optimal player\n",
    "                    state = get_state(grid, 1 - w%2)\n",
    "                    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[w%2]) #1 if rand 0 if opt\n",
    "                    for t in range(9):\n",
    "                        if env.current_player == player_opt_1.player:\n",
    "                            move = convert(player_opt_1.act(grid))\n",
    "                            grid, end, winner = env.step(move, print_grid=False)\n",
    "                        else:\n",
    "                            # Select an action according to policy_net, setting eps = 0 and target = True\n",
    "                            move = select_action(get_state(grid, 1-w%2), eps=0, target = True)\n",
    "                            # Try the chosen action\n",
    "                            try:\n",
    "                                grid, end, winner = env.step(int(move), print_grid=False)\n",
    "                            except ValueError:\n",
    "                                current_testing.append(Turns[w%2])\n",
    "                                break\n",
    "\n",
    "                        if end:\n",
    "                            current_testing.append(winner)\n",
    "                            break\n",
    "                winners.append(current_testing)\n",
    "            # testing ended\n",
    "            # Initialize the environment and state\n",
    "            print(\"iteration\", repetition,\", game \"+str(i))\n",
    "            eps = max(min_eps, max_eps*(1-((i)/n_star))) # varying epsilon\n",
    "            env.reset()\n",
    "            grid, _, __ = env.observe()\n",
    "            # Set current state and optimal player \n",
    "            state = get_state(grid, 1 - i%2)\n",
    "            player_opt_1 = OptimalPlayer(epsilon=epsilon, player=Turns[i%2])\n",
    "            for t in range(9):\n",
    "                if env.current_player == player_opt_1.player:\n",
    "                    move = convert(player_opt_1.act(grid))\n",
    "                    grid, end, winner = env.step(move, print_grid=False)\n",
    "                else:\n",
    "                    # Select and try an action\n",
    "                    move = select_action(state, eps)\n",
    "                    last_move_q = move\n",
    "                    try:\n",
    "                        last_state_q = get_state(grid, 1 - i%2)\n",
    "                        grid, end, winner = env.step(int(move), print_grid=False)\n",
    "                        reward = torch.Tensor([env.reward(player=Turns[1 - i%2])])\n",
    "                    except ValueError:\n",
    "                        reward = torch.Tensor([-1])\n",
    "                        memory.push(last_state_q, last_move_q, None, reward)\n",
    "                        break\n",
    "\n",
    "                # When it's Q-player's turn and it's not the first move neither the end, push to memory buffer\n",
    "                if not end:\n",
    "                    next_state = get_state(grid, 1-i%2)\n",
    "                    if env.current_player != player_opt_1.player and t>0:\n",
    "                        memory.push(last_state_q, last_move_q, next_state, reward)\n",
    "\n",
    "                # If game has ended, push to memory, set to None the next state\n",
    "                else:\n",
    "                    next_state = None\n",
    "                    reward = env.reward(player=Turns[1 - i%2])\n",
    "                    memory.push(last_state_q, last_move_q, next_state, torch.Tensor([reward]))\n",
    "                    break\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                loss = optimize_model(policy_net, memory, optimizer)\n",
    "\n",
    "            # Update the target network, copying all weights and biases from policy_net\n",
    "            if i % 500 == 0:\n",
    "                print(\"loaded\")\n",
    "                target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations of $M_{rand}$ and $M_{opt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epsilon in [0, 0.1, 0.25, 0.5, 0.75, 0.9]:  # Different vaues of epsilon to try \n",
    "    results = []\n",
    "    for repetition in range(1):\n",
    "        # Initialize parameters and the neural networks\n",
    "        BATCH_SIZE = 64\n",
    "        GAMMA = 0.99\n",
    "        TARGET_UPDATE = 500\n",
    "\n",
    "        n_actions = 9\n",
    "\n",
    "        policy_net = DQN().to(device)\n",
    "        target_net = DQN().to(device)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "\n",
    "        # Initialize optimizer and memory buffer\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n",
    "        memory = ReplayMemory(10000)\n",
    "\n",
    "        num_games = 20000\n",
    "        Turns = np.array(['X','O'])\n",
    "        winners = []\n",
    "        for i in range(1, num_games+1):\n",
    "            # Testing phase\n",
    "            if i%250==0:\n",
    "                print(\"Testing...\")\n",
    "                current_testing = []\n",
    "                for w in range(500):\n",
    "                    #Reset the environment\n",
    "                    env.reset()\n",
    "                    grid, _, __ = env.observe()\n",
    "                    # Set current state and optimal player\n",
    "                    state = get_state(grid, 1 - w%2)\n",
    "                    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[w%2]) #1 if rand 0 if opt\n",
    "                    for t in range(9):\n",
    "                        if env.current_player == player_opt_1.player:\n",
    "                            move = convert(player_opt_1.act(grid))\n",
    "                            grid, end, winner = env.step(move, print_grid=False)\n",
    "                        else:\n",
    "                            # Select and try an action\n",
    "                            move = select_action(get_state(grid, 1-w%2), eps=0, target = True)\n",
    "                            try:\n",
    "                                grid, end, winner = env.step(int(move), print_grid=False)\n",
    "                            except ValueError:\n",
    "                                current_testing.append(Turns[w%2])\n",
    "                                break\n",
    "\n",
    "                        if end:\n",
    "                            current_testing.append(winner)\n",
    "                            break\n",
    "                winners.append(current_testing)\n",
    "            # Testing ended\n",
    "            # Initialize the environment and state\n",
    "            print(\"iteration\", repetition,\", game \"+str(i))\n",
    "            env.reset()\n",
    "            grid, _, __ = env.observe()\n",
    "            # Set the states from both Q-players' perspectives\n",
    "            state_1 = get_state(grid, i%2)\n",
    "            state_2 = get_state(grid, 1 - i%2)\n",
    "            for t in range(9):\n",
    "                # For each player, propose a move, try it, and save it and the state as last move and last state\n",
    "                if env.current_player == Turns[i%2]:\n",
    "                    move = select_action(state_1, epsilon)\n",
    "                    last_move_1 = move\n",
    "                    try:\n",
    "                        # Save the state from current palyer's view\n",
    "                        last_state_1 = get_state(grid, i%2)\n",
    "                        grid, end, winner = env.step(int(move), print_grid=False)\n",
    "                        # Save current player's reward\n",
    "                        reward1 = torch.Tensor([env.reward(player=Turns[i%2])])\n",
    "                    except ValueError:\n",
    "                        # Set current player's reward to -1, push to memory and end the game\n",
    "                        reward1 = torch.Tensor([-1])\n",
    "                        memory.push(last_state_1, last_move_1, None, reward1)\n",
    "                        break\n",
    "                else:\n",
    "                    move = select_action(state_2, epsilon)\n",
    "                    last_move_2 = move\n",
    "                    try:\n",
    "                        # Save the state from current palyer's view\n",
    "                        last_state_2 = get_state(grid, 1 - i%2)\n",
    "                        grid, end, winner = env.step(int(move), print_grid=False)\n",
    "                        # Save current player's reward\n",
    "                        reward2 = torch.Tensor([env.reward(player=Turns[1 - i%2])])\n",
    "                    except ValueError:\n",
    "                        # Set current player's reward to -1, push to memory and end the game\n",
    "                        reward2 = torch.Tensor([-1])\n",
    "                        memory.push(last_state_2, last_move_2, None, reward2)\n",
    "                        break\n",
    "\n",
    "                # If the game has not ended, push the transition of the current player\n",
    "                if not end:\n",
    "                    if env.current_player == Turns[i%2]:\n",
    "                        next_state = get_state(grid, i%2)\n",
    "                        memory.push(last_state_1, last_move_1, next_state, reward1)\n",
    "                    elif env.current_player == Turns[1-i%2]:\n",
    "                        next_state = get_state(grid, 1-i%2)\n",
    "                        memory.push(last_state_2, last_move_2, next_state, reward2)\n",
    "\n",
    "                # If the game has ended push both players' transitions with next state = None\n",
    "                else:\n",
    "                    next_state = None\n",
    "                    memory.push(last_state_2, last_move_2, next_state, torch.Tensor([env.reward(player=Turns[1 - i%2])]))\n",
    "                    memory.push(last_state_1, last_move_1, next_state, torch.Tensor([env.reward(player=Turns[i%2])]))\n",
    "                    break\n",
    "\n",
    "                # Update states\n",
    "                state_1 = get_state(grid, i%2)\n",
    "                state_2 = get_state(grid, 1 - i%2)\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                loss = optimize_model(policy_net, memory, optimizer)\n",
    "\n",
    "            # Update the target network, copying all weights and biases from policy_net\n",
    "            if i % 500 == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in [1000, 5000, 10000, 15000, 20000, 40000]: # Different vaues of n* to try \n",
    "    results = []\n",
    "    for repetition in range(1):\n",
    "        # Initialize parameters and the neural networks\n",
    "        BATCH_SIZE = 64\n",
    "        GAMMA = 0.99\n",
    "        max_eps = 0.8\n",
    "        min_eps = 0.1\n",
    "        n_star = n\n",
    "        TARGET_UPDATE = 500\n",
    "\n",
    "        n_actions = 9\n",
    "\n",
    "        policy_net = DQN().to(device)\n",
    "        target_net = DQN().to(device)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "\n",
    "        # Initialize optimizer and memory buffer\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n",
    "        memory = ReplayMemory(10000)\n",
    "\n",
    "        num_games = 20000\n",
    "        Turns = np.array(['X','O'])\n",
    "        winners = []\n",
    "        for i in range(1, num_games+1):\n",
    "            # Testing phase\n",
    "            if i%250==0:\n",
    "                print(\"Testing...\")\n",
    "                current_testing = []\n",
    "                for w in range(500):\n",
    "                    #Reset the environment\n",
    "                    env.reset()\n",
    "                    grid, _, __ = env.observe()\n",
    "                    # Set current state and optimal player\n",
    "                    state = get_state(grid, 1 - w%2)\n",
    "                    player_opt_1 = OptimalPlayer(epsilon=1., player=Turns[w%2]) #1 if rand 0 if opt\n",
    "                    for t in range(9):\n",
    "                        if env.current_player == player_opt_1.player:\n",
    "                            move = convert(player_opt_1.act(grid))\n",
    "                            grid, end, winner = env.step(move, print_grid=False)\n",
    "                        else:\n",
    "                            # Select and try an action\n",
    "                            move = select_action(get_state(grid, 1-w%2), eps=0, target = True)\n",
    "                            try:\n",
    "                                grid, end, winner = env.step(int(move), print_grid=False)\n",
    "                            except ValueError:\n",
    "                                current_testing.append(Turns[w%2])\n",
    "                                break\n",
    "\n",
    "                        if end:\n",
    "                            current_testing.append(winner)\n",
    "                            break\n",
    "                winners.append(current_testing)\n",
    "            # Testing ended\n",
    "            # Initialize the environment and state\n",
    "            print(\"iteration\", repetition,\", game \"+str(i))\n",
    "            env.reset()\n",
    "            epsilon = max(min_eps, max_eps*(1-((i)/n_star)))\n",
    "            grid, _, __ = env.observe()\n",
    "            # Set the states from both Q-players' perspectives\n",
    "            state_1 = get_state(grid, i%2)\n",
    "            state_2 = get_state(grid, 1 - i%2)\n",
    "            for t in range(9):\n",
    "                # For each player, propose a move, try it, and save it and the state as last move and last state\n",
    "                if env.current_player == Turns[i%2]:\n",
    "                    move = select_action(state_1, epsilon)\n",
    "                    last_move_1 = move\n",
    "                    try:\n",
    "                        # Save the state from current palyer's view\n",
    "                        last_state_1 = get_state(grid, i%2)\n",
    "                        last_state_1_rev = get_state(grid, 1 - i%2)\n",
    "                        grid, end, winner = env.step(int(move), print_grid=False)\n",
    "                        # Save current player's reward\n",
    "                        reward1 = torch.Tensor([env.reward(player=Turns[i%2])])\n",
    "                    except ValueError:\n",
    "                        # Set current player's reward to -1, push to memory and end the game\n",
    "                        reward1 = torch.Tensor([-1])\n",
    "                        memory.push(last_state_1, last_move_1, None, reward1)\n",
    "                        break\n",
    "                else:\n",
    "                # Select and perform an action\n",
    "                    move = select_action(state_2, epsilon)\n",
    "                    last_move_2 = move\n",
    "                    try:\n",
    "                        # Save the state from current palyer's view\n",
    "                        last_state_2 = get_state(grid, 1 - i%2)\n",
    "                        last_state_2_rev = get_state(grid, i%2)\n",
    "                        grid, end, winner = env.step(int(move), print_grid=False)\n",
    "                        # Save current player's reward\n",
    "                        reward2 = torch.Tensor([env.reward(player=Turns[1 - i%2])])\n",
    "                    except ValueError:\n",
    "                        # Set current player's reward to -1, push to memory and end the game\n",
    "                        reward2 = torch.Tensor([-1])\n",
    "                        memory.push(last_state_2, last_move_2, None, reward2)\n",
    "                        break\n",
    "\n",
    "                # If the game has not ended, push the transition of the current player\n",
    "                if not end:\n",
    "                    if env.current_player == Turns[i%2]:\n",
    "                        next_state = get_state(grid, i%2)\n",
    "                        memory.push(last_state_1, last_move_1, next_state, reward1)\n",
    "                    elif env.current_player == Turns[1-i%2]:\n",
    "                        next_state = get_state(grid, 1-i%2)\n",
    "                        memory.push(last_state_2, last_move_2, next_state, reward2)\n",
    "\n",
    "                # If the game has ended push both players' transitions with next state = None\n",
    "                else:\n",
    "                    next_state = None\n",
    "                    reward = env.reward(player=Turns[1 - i%2])\n",
    "                    memory.push(last_state_2, last_move_2, next_state, torch.Tensor([reward]))\n",
    "                    memory.push(last_state_1, last_move_1, next_state, torch.Tensor([env.reward(player=Turns[i%2])]))\n",
    "                    break\n",
    "\n",
    "                # Update states\n",
    "                state_1 = get_state(grid, i%2)\n",
    "                state_2 = get_state(grid, 1 - i%2)\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                loss = optimize_model(policy_net, memory, optimizer)\n",
    "\n",
    "            # Update the target network, copying all weights and biases from policy_net\n",
    "            if i % 500 == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        results.append(winners)\n",
    "        np.save(f\"Q17_rand_n{n}_1run\", results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "tic_tac_toe.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
